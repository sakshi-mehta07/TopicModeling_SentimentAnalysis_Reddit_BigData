{"cells":[{"cell_type":"markdown","id":"cd5064a8","metadata":{},"source":["# Topic Modelling using BERTopic\n","In this notebook, BERTopic was used to perform topic modeling on a subset of the Reddit Climate Change dataset, sourced from: \\url{https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset}. To select an appropriate embedding model, reference was made to the MTEB leaderboard (\\url{https://huggingface.co/spaces/mteb/leaderboard}), prioritizing models that balance performance and efficiency. After constructing and fitting a custom BERTopic pipeline, the Groq API was integrated to generate more descriptive and human-readable topic labels, enhancing the interpretability of the resulting topics."]},{"cell_type":"markdown","id":"531e71bd","metadata":{},"source":["**GCP Cluster specifications used**:\n","gcloud dataproc clusters create st446-cluster-gp2 \\\n","  --enable-component-gateway \\\n","  --public-ip-address \\\n","  --region europe-west1 \\\n","  --master-machine-type n2-standard-16 \\\n","  --master-boot-disk-size 100 \\\n","  --num-workers 2 \\\n","  --worker-machine-type n2-standard-2 \\\n","  --worker-boot-disk-size 200 \\\n","  --image-version 2.2-debian12 \\\n","  --optional-components JUPYTER \\\n","  --metadata 'PIP_PACKAGES=sklearn nltk pandas numpy' \\\n","  --project st446wt2025"]},{"cell_type":"markdown","id":"8b646f1c","metadata":{},"source":["## Notebook configurations and data loading"]},{"cell_type":"code","execution_count":1,"id":"79b3fc11","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bertopic\n","  Downloading bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n","Collecting sentence-transformers\n","  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n","Collecting transformers\n","  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n","Collecting umap-learn\n","  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n","Collecting hdbscan\n","  Downloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n","Requirement already satisfied: numpy>=1.20.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic) (1.26.4)\n","Requirement already satisfied: pandas>=1.1.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic) (2.1.4)\n","Collecting plotly>=4.7.0 (from bertopic)\n","  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: scikit-learn>=1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic) (1.3.2)\n","Requirement already satisfied: tqdm>=4.41.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic) (4.65.0)\n","Collecting torch>=1.11.0 (from sentence-transformers)\n","  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Requirement already satisfied: scipy in /opt/conda/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n","Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n","  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: Pillow in /opt/conda/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (10.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (4.13.2)\n","Requirement already satisfied: filelock in /opt/conda/miniconda3/lib/python3.11/site-packages (from transformers) (3.18.0)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.22,>=0.21 (from transformers)\n","  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting safetensors>=0.4.3 (from transformers)\n","  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: numba>=0.51.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from umap-learn) (0.58.1)\n","Collecting pynndescent>=0.5 (from umap-learn)\n","  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: joblib>=1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from hdbscan) (1.4.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.12.2)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n","Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from plotly>=4.7.0->bertopic) (1.36.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n","Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: networkx in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.3.0 (from torch>=1.11.0->sentence-transformers)\n","  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers) (68.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Downloading bertopic-0.17.0-py3-none-any.whl (150 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m150.6/150.6 kB\u001B[0m \u001B[31m21.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m345.7/345.7 kB\u001B[0m \u001B[31m40.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.4/10.4 MB\u001B[0m \u001B[31m195.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.8/88.8 kB\u001B[0m \u001B[31m15.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.6/4.6 MB\u001B[0m \u001B[31m18.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m:00:01\u001B[0m\n","\u001B[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m481.4/481.4 kB\u001B[0m \u001B[31m61.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.8/14.8 MB\u001B[0m \u001B[31m175.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.9/56.9 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m471.6/471.6 kB\u001B[0m \u001B[31m53.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m153.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m865.2/865.2 MB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m393.1/393.1 MB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.9/8.9 MB\u001B[0m \u001B[31m180.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m100.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m897.7/897.7 kB\u001B[0m \u001B[31m77.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m571.0/571.0 MB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m200.2/200.2 MB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m83.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m27.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m158.2/158.2 MB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m216.6/216.6 MB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m156.8/156.8 MB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m201.3/201.3 MB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.7/19.7 MB\u001B[0m \u001B[31m11.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m89.3/89.3 kB\u001B[0m \u001B[31m189.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n","\u001B[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m156.5/156.5 MB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m\n","\u001B[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, safetensors, plotly, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, pynndescent, nvidia-cusolver-cu12, hdbscan, umap-learn, transformers, torch, sentence-transformers, bertopic\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.12.1\n","    Uninstalling sympy-1.12.1:\n","      Successfully uninstalled sympy-1.12.1\n","Successfully installed bertopic-0.17.0 hdbscan-0.8.40 huggingface-hub-0.30.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 plotly-6.0.1 pynndescent-0.5.13 safetensors-0.5.3 sentence-transformers-4.1.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 transformers-4.51.3 triton-3.3.0 umap-learn-0.5.7\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mCollecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from gensim) (1.11.4)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting smart-open>=1.8.1 (from gensim)\n","  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n","Collecting wrapt (from smart-open>=1.8.1->gensim)\n","  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m0:01\u001B[0m\n","\u001B[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.7/61.7 kB\u001B[0m \u001B[31m71.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n","\u001B[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.2/83.2 kB\u001B[0m \u001B[31m85.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n","\u001B[?25hInstalling collected packages: wrapt, smart-open, gensim\n","Successfully installed gensim-4.3.3 smart-open-7.1.0 wrapt-1.17.2\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: bertopic[spacy] in /opt/conda/miniconda3/lib/python3.11/site-packages (0.17.0)\n","Requirement already satisfied: hdbscan>=0.8.29 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic[spacy]) (0.8.40)\n","Requirement already satisfied: numpy>=1.20.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic[spacy]) (1.26.4)\n","Requirement already satisfied: pandas>=1.1.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic[spacy]) (2.1.4)\n","Requirement already satisfied: plotly>=4.7.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic[spacy]) (6.0.1)\n","Requirement already satisfied: scikit-learn>=1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic[spacy]) (1.3.2)\n","Requirement already satisfied: sentence-transformers>=0.4.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic[spacy]) (4.1.0)\n","Requirement already satisfied: tqdm>=4.41.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic[spacy]) (4.65.0)\n","Requirement already satisfied: umap-learn>=0.5.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from bertopic[spacy]) (0.5.7)\n","Collecting spacy>=3.0.1 (from bertopic[spacy])\n","  Downloading spacy-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n","Requirement already satisfied: scipy>=1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from hdbscan>=0.8.29->bertopic[spacy]) (1.11.4)\n","Requirement already satisfied: joblib>=1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from hdbscan>=0.8.29->bertopic[spacy]) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic[spacy]) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic[spacy]) (2025.2)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic[spacy]) (2025.2)\n","Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from plotly>=4.7.0->bertopic[spacy]) (1.36.0)\n","Requirement already satisfied: packaging in /opt/conda/miniconda3/lib/python3.11/site-packages (from plotly>=4.7.0->bertopic[spacy]) (23.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from scikit-learn>=1.0->bertopic[spacy]) (3.6.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (4.51.3)\n","Requirement already satisfied: torch>=1.11.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (2.7.0)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (0.30.2)\n","Requirement already satisfied: Pillow in /opt/conda/miniconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (10.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (4.13.2)\n","Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n","Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n","Collecting murmurhash<1.1.0,>=0.28.0 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n","Collecting cymem<2.1.0,>=2.0.2 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n","Collecting preshed<3.1.0,>=3.0.2 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n","Collecting thinc<8.4.0,>=8.3.4 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n","Collecting wasabi<1.2.0,>=0.9.1 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n","Collecting srsly<3.0.0,>=2.4.3 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Collecting catalogue<2.1.0,>=2.0.6 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n","Collecting weasel<0.5.0,>=0.1.0 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n","Collecting typer<1.0.0,>=0.3.0 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from spacy>=3.0.1->bertopic[spacy]) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/miniconda3/lib/python3.11/site-packages (from spacy>=3.0.1->bertopic[spacy]) (2.11.3)\n","Requirement already satisfied: jinja2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from spacy>=3.0.1->bertopic[spacy]) (3.1.6)\n","Requirement already satisfied: setuptools in /opt/conda/miniconda3/lib/python3.11/site-packages (from spacy>=3.0.1->bertopic[spacy]) (68.2.2)\n","Collecting langcodes<4.0.0,>=3.2.0 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: numba>=0.51.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from umap-learn>=0.5.0->bertopic[spacy]) (0.58.1)\n","Requirement already satisfied: pynndescent>=0.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from umap-learn>=0.5.0->bertopic[spacy]) (0.5.13)\n","Requirement already satisfied: filelock in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[spacy]) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[spacy]) (2023.12.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[spacy]) (6.0.2)\n","Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->bertopic[spacy])\n","  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic[spacy]) (0.41.1)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[spacy]) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[spacy]) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[spacy]) (0.4.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic[spacy]) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[spacy]) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[spacy]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[spacy]) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[spacy]) (2025.1.31)\n","Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->bertopic[spacy])\n","  Downloading blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n","Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->bertopic[spacy])\n","  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n","INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n","Collecting thinc<8.4.0,>=8.3.4 (from spacy>=3.0.1->bertopic[spacy])\n","  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n","Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->bertopic[spacy])\n","  Downloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n","Requirement already satisfied: sympy>=1.13.3 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (1.14.0)\n","Requirement already satisfied: networkx in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (3.4.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (1.11.1.6)\n","Requirement already satisfied: triton==3.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (3.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (0.5.3)\n","Requirement already satisfied: click>=8.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (8.1.8)\n","Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy])\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: rich>=10.11.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (14.0.0)\n","Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->bertopic[spacy])\n","  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->bertopic[spacy]) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from jinja2->spacy>=3.0.1->bertopic[spacy]) (3.0.2)\n","Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->bertopic[spacy])\n","  Downloading marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (2.19.1)\n","Requirement already satisfied: wrapt in /opt/conda/miniconda3/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.1->bertopic[spacy]) (1.17.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (0.1.2)\n"]},{"name":"stdout","output_type":"stream","text":["Downloading spacy-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.6/30.6 MB\u001B[0m \u001B[31m68.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n","Downloading cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m218.9/218.9 kB\u001B[0m \u001B[31m33.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m183.0/183.0 kB\u001B[0m \u001B[31m28.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.3/134.3 kB\u001B[0m \u001B[31m24.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m157.2/157.2 kB\u001B[0m \u001B[31m27.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n","Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n","Downloading srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m98.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.9/3.9 MB\u001B[0m \u001B[31m146.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading typer-0.15.3-py3-none-any.whl (45 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.3/45.3 kB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n","Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.3/50.3 kB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.7/11.7 MB\u001B[0m \u001B[31m69.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m52.7/52.7 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n","Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.4/5.4 MB\u001B[0m \u001B[31m146.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Downloading marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m51.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hInstalling collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n","Successfully installed blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.3 wasabi-1.1.3 weasel-0.4.1\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mCollecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m145.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hInstalling collected packages: en-core-web-sm\n","Successfully installed en-core-web-sm-3.8.0\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Collecting groq\n","  Downloading groq-0.24.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from groq) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from groq) (1.8.0)\n","Collecting httpx<1,>=0.23.0 (from groq)\n","  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from groq) (2.11.3)\n","Requirement already satisfied: sniffio in /opt/conda/miniconda3/lib/python3.11/site-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/conda/miniconda3/lib/python3.11/site-packages (from groq) (4.13.2)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/miniconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n","Requirement already satisfied: certifi in /opt/conda/miniconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n","  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n","Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n","  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n","Downloading groq-0.24.0-py3-none-any.whl (127 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.5/127.5 kB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.5/73.5 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.8/78.8 kB\u001B[0m \u001B[31m14.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n","Installing collected packages: h11, httpcore, httpx, groq\n","Successfully installed groq-0.24.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: huggingface_hub[hf_xet] in /opt/conda/miniconda3/lib/python3.11/site-packages (0.30.2)\n","Requirement already satisfied: filelock in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (2023.12.2)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n","Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (4.65.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface_hub[hf_xet]) (4.13.2)\n","Collecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet])\n"]},{"name":"stdout","output_type":"stream","text":["  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub[hf_xet]) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub[hf_xet]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub[hf_xet]) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n","Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.6/53.6 MB\u001B[0m \u001B[31m30.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hInstalling collected packages: hf-xet\n","Successfully installed hf-xet-1.1.0\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["# Install required packages\n","!pip install bertopic sentence-transformers transformers umap-learn hdbscan\n","#!pip install --upgrade bertopic\n","!pip install gensim\n","!pip install bertopic[spacy]\n","!python -m spacy download en_core_web_sm\n","!pip install groq\n","!pip install 'huggingface_hub[hf_xet]'"]},{"cell_type":"code","execution_count":7,"id":"bbc6bf0d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 1536M  100 1536M    0     0  42.4M      0  0:00:36  0:00:36 --:--:-- 42.8M\n","Archive:  climate.zip\n","  inflating: the-reddit-climate-change-dataset-comments.csv  \n","  inflating: the-reddit-climate-change-dataset-posts.csv  \n","Deleted /the-reddit-climate-change-dataset-comments.csv\n","Found 4 items\n","-rw-r--r--   2 root hadoop 4111000325 2025-05-03 11:31 /the-reddit-climate-change-dataset-comments.csv\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-03 11:17 /tmp\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-03 11:18 /user\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-03 11:17 /var\n"]}],"source":["# Download the Kaggle dataset zip\n","!curl -L -o climate.zip \\\n","    \"https://www.kaggle.com/api/v1/datasets/download/pavellexyr/the-reddit-climate-change-dataset\"\n","\n","# Unzip it\n","!unzip -o climate.zip\n","\n","# Remove any old copy in HDFS and put the comments file there\n","!hadoop fs -rm -f /the-reddit-climate-change-dataset-comments.csv\n","!hadoop fs -put the-reddit-climate-change-dataset-comments.csv /\n","\n","# Verify upload\n","!hadoop fs -ls /"]},{"cell_type":"code","execution_count":2,"id":"7f589c33","metadata":{},"outputs":[],"source":["# Import libraries used in this notebook\n","import zipfile\n","import sys\n","import os\n","import re\n","import hashlib\n","from datetime import datetime\n","import time\n","import numpy as np\n","import pandas as pd\n","import string\n","import spacy\n","import groq\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n","from sentence_transformers import SentenceTransformer, models\n","from bertopic import BERTopic\n","from bertopic.representation import MaximalMarginalRelevance\n","from bertopic.representation import KeyBERTInspired\n","from sklearn.feature_extraction.text import CountVectorizer\n","from gensim.models import CoherenceModel\n","from gensim.corpora import Dictionary\n","from umap import UMAP\n","from hdbscan import HDBSCAN\n","from sklearn.feature_extraction.text import CountVectorizer\n","from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech, OpenAI\n","from huggingface_hub import HfFileSystem\n","from collections import Counter\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":3,"id":"3d5a8e1b","metadata":{},"outputs":[],"source":["# Set HDFS path where file is saved\n","comments_path = \"hdfs://st446-cluster-gp2-m:8020/the-reddit-climate-change-dataset-comments.csv\""]},{"cell_type":"code","execution_count":4,"id":"112c4673","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- type: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- subreddit.id: string (nullable = true)\n"," |-- subreddit.name: string (nullable = true)\n"," |-- subreddit.nsfw: string (nullable = true)\n"," |-- created_utc: string (nullable = true)\n"," |-- permalink: string (nullable = true)\n"," |-- body: string (nullable = true)\n"," |-- sentiment: double (nullable = true)\n"," |-- score: integer (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","|   type|     id|subreddit.id|subreddit.name|subreddit.nsfw|created_utc|           permalink|                body|sentiment|score|\n","+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","|comment|imlddn9|       2qh3l|          news|         false| 1661990368|https://old.reddi...|Yeah but what the...|   0.5719|    2|\n","|comment|imldbeh|       2qn7b|          ohio|         false| 1661990340|https://old.reddi...|Any comparison of...|  -0.9877|    2|\n","|comment|imldado|       2qhma|    newzealand|         false| 1661990327|https://old.reddi...|I'm honestly wait...|  -0.1143|    1|\n","|comment|imld6cb|       2qi09|    sacramento|         false| 1661990278|https://old.reddi...|Not just Sacramen...|      0.0|    4|\n","|comment|imld0kj|       2qh1i|     askreddit|         false| 1661990206|https://old.reddi...|I think climate c...|   0.6634|    1|\n","+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","only showing top 5 rows\n","\n"]}],"source":["# Define the schema to read the comments file\n","\n","schema = StructType([\n","    StructField(\"type\",           StringType(), True),\n","    StructField(\"id\",             StringType(), True),\n","    StructField(\"subreddit.id\",   StringType(), True),\n","    StructField(\"subreddit.name\", StringType(), True),\n","    StructField(\"subreddit.nsfw\", StringType(), True),\n","    StructField(\"created_utc\",    StringType(), True),\n","    StructField(\"permalink\",      StringType(), True),\n","    StructField(\"body\",           StringType(), True),\n","    StructField(\"sentiment\",      DoubleType(), True),\n","    StructField(\"score\",          IntegerType(),True)\n","])\n","\n","df = spark.read \\\n","    .option(\"header\", \"true\") \\\n","    .option(\"multiLine\", \"true\") \\\n","    .option(\"escape\", \"\\\"\") \\\n","    .schema(schema) \\\n","    .csv(comments_path)\n","\n","df.printSchema()\n","df.show(5)"]},{"cell_type":"markdown","id":"93681c01","metadata":{},"source":["## BERTopic pipeline"]},{"cell_type":"markdown","id":"1b863df0","metadata":{},"source":["BERT works with different submodels that can be changed and tuned:\n","1. **Embedding models**: Taking into account the restrictions in computational power and the MTEB Leaderboard (https://huggingface.co/spaces/mteb/leaderboard), we chose the following embedding models 'all-MiniLM-L6-v2'. Bigger models such as e.g. 'BAAI/bge-base-en-v1.5' might yield better results, but we were unable to run them on a reasonable subset of comments given our cluster only has a master-machine-type n2-standard-16 and we don't have access to a GPU because we're using a free-tier GC account.\n","\n","For the additional submodels in BERT, we followed the best practices of the official website: https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html#additional-representations\n","\n","2. **Representation Models**: For representation models, we used an ensemble model including: Keybert Model and Part of Speech (POS). Using them together in the pipeline, BERTopic internally combined the outputs of the two models, fusioning the best keywords based on the quality and diversity of the words returned by all models.\n","3. **Vectorizer Models**: As BERT does not perform any preprocessing of the documents (e.g. tokenization, stopword removal, lemmatization), CountVectorizer is applied to remove stopwords, ignore infrequent words and increase the n-gram range after documents are assigned to topics.\n","4. **Dimensionality Reduction Models**: UMAP is used by default in BERT to reduce the dimensionality of the embeddings. To be able to recreate the exact same results, we will specify the model and set a random state to deal with its stochastic behaviour.\n","5. **Cluster Model**: The cluster model is by default HDBSCAN. HDBSCAN has a parameter (min_cluster_size) that indirectly controls the number of topics that will be created. We will set that parameter to 150 to avoid the creation of too many small clusters. \n","\n","The following metrics were used to evaluate model performance: \n","**(I) Topic Modeling Evaluation Metrics**, including C\\_V, U\\_Mass and C\\_NPMI coherence, Topic imbalance, Topic Diversity and **(II) Metrics for Distributed Computing}**, including Runtime and Datasetsize. \n","\n","\n","We tried different hyperparameter configurations for UMAP and HDBSCAN on a subset of 10,000 comments and then chose the best performing combination to based on the evaluation metrics to be run on a subset of 100,000 comments."]},{"cell_type":"code","execution_count":30,"id":"36b960d7","metadata":{},"outputs":[],"source":["# Select a subset of comments from the dataframe\n","documents = df.select(\"body\").limit(10000).rdd.flatMap(lambda x: x).collect()\n","\n","# Keep only posts with at least 5 meaningful words\n","documents_clean = [doc for doc in documents if len(doc.split()) >= 5]"]},{"cell_type":"code","execution_count":31,"id":"cbe1a0e6","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6d2355e093c4df59b4f081a5b488220","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/307 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Pre-calculate embeddings to avoid recalculating each time\n","embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n","MiniLM_embeddings = embedding_model.encode(documents_clean, show_progress_bar=True)"]},{"cell_type":"code","execution_count":5,"id":"5cb336fe","metadata":{},"outputs":[],"source":["# Define funtion to compute coherence score\n","def compute_coherence_score(documents, topic_model, top_n_words=10):\n","    tokenized_documents = [doc.split() for doc in documents]\n","    dictionary = Dictionary(tokenized_documents)\n","    corpus = [dictionary.doc2bow(text) for text in tokenized_documents]\n","\n","    topic_words = []\n","    for topic_id, topic in topic_model.get_topics().items():\n","        if topic_id == -1:\n","            continue\n","        words = [word for word, _ in topic[:top_n_words]]\n","        topic_words.append(words)\n","\n","    coherence_model = CoherenceModel(\n","        topics=topic_words,\n","        texts=tokenized_documents,\n","        dictionary=dictionary,\n","        coherence='c_v'\n","    )\n","    coherence_score = coherence_model.get_coherence()\n","    print(f\"Topic Coherence (C_V): {coherence_score:.4f}\")\n","    return coherence_score\n","\n","# Other Coherence metrics\n","def calculate_umass_npmi(documents, topic_model):\n","    # Step 1: Tokenize your documents\n","    tokenized_documents = [doc.split() for doc in documents]\n","\n","    # Step 2: Create a dictionary and corpus\n","    dictionary = Dictionary(tokenized_documents)\n","    corpus = [dictionary.doc2bow(text) for text in tokenized_documents]\n","\n","    # Step 3: Extract topics from BERTopic\n","    topic_words = []\n","    for topic in topic_model.get_topics().values():\n","        topic_words.append([word for word, _ in topic])\n","        \n","    # Calculate UMASS coherence\n","    BERTopic_umass = CoherenceModel(\n","        topics=topic_words,\n","        texts=tokenized_documents,\n","        dictionary=dictionary,\n","        coherence=\"u_mass\"\n","    ).get_coherence()\n","\n","    print(f\"BERTopic U_Mass coherence  = {BERTopic_umass:.4f}\")\n","    \n","    # Calculate C_NPMI coherence\n","    BERTopic_npmi = CoherenceModel(\n","        topics=topic_words,\n","        texts=tokenized_documents,\n","        dictionary=dictionary,\n","        coherence=\"c_npmi\"\n","    ).get_coherence()\n","\n","    print(f\"BERTopic C_NPMI coherence  = {BERTopic_npmi:.4f}\")\n","    return BERTopic_umass, BERTopic_npmi\n","\n","# Define function to calculate Topic Diversity\n","def calculate_topic_diversity(topic_model, top_n_words=10):\n","    # 1. Pull top words per topic\n","    topics = topic_model.get_topics()\n","    \n","    top_words_per_topic = []\n","    for topic_id, words_scores in topics.items():\n","        # Skip outlier topic (-1)\n","        if topic_id == -1:\n","            continue\n","        words = [word for word, _ in words_scores[:top_n_words]]\n","        top_words_per_topic.append(words)\n","    \n","    # 2. Flatten and count uniques\n","    all_top_words = [word for topic in top_words_per_topic for word in topic]\n","    unique_words = set(all_top_words)\n","\n","    # 3. Compute diversity\n","    diversity = len(unique_words) / len(all_top_words)\n","\n","    print(f\"BERTopic diversity = {diversity:.4f}  \"\n","          f\"({len(unique_words)} unique of {len(all_top_words)} total words)\")\n","    \n","    return diversity, len(unique_words), len(all_top_words)\n","\n","# Define function to calculate topic size imbalance\n","def calculate_topic_size_imbalance(topics):\n","    # Remove noise topic (-1)\n","    filtered_topics = [topic for topic in topics if topic != -1]\n","\n","    # Count documents per topic\n","    topic_counts = Counter(filtered_topics)\n","\n","    if len(topic_counts) <= 1:\n","        print(\"Not enough topics to compute imbalance.\")\n","        return None\n","\n","    max_size = max(topic_counts.values())\n","    min_size = min(topic_counts.values())\n","    imbalance = max_size / min_size\n","\n","    print(f\"BERTopic Topic size imbalance (max/min): {imbalance:.2f}\")\n","    return imbalance\n","\n","# Define pipeline functions to get topic info and calculate coherence score for different embeddings\n","\n","def pipeline(embeddings):\n","    # Set seed to avoid randomness in UMAP dimensionality reduction\n","    umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine', random_state=42)\n","    \n","    # Use HDBSCAN model to control the number of topics\n","    hdbscan_model = HDBSCAN(min_cluster_size=50, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n","    \n","    # Preprocess the topic representations after documents are assigned to topics to not influence the clustering process\n","    vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n","    \n","    # Add representations\n","    # KeyBERT\n","    keybert_model = KeyBERTInspired()\n","\n","    # Part-of-Speech\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    pos_model = PartOfSpeech(nlp)\n","\n","\n","    # Ensemble representation model\n","    representation_model = {\n","        \"KeyBERT\": keybert_model,\n","        \"POS\": pos_model\n","    }\n","    \n","    topic_model = BERTopic(\n","\n","      # Pipeline models\n","      embedding_model=embedding_model,\n","      umap_model=umap_model,\n","      hdbscan_model=hdbscan_model,\n","      vectorizer_model=vectorizer_model,\n","      representation_model=representation_model,\n","\n","      # Hyperparameters\n","      top_n_words=10,\n","      verbose=True\n","    )\n","\n","    # Train model\n","    topics, probs = topic_model.fit_transform(documents_clean, embeddings)\n","    \n","    # Compute metrics\n","    cv_final = compute_coherence_score(documents=documents_clean, topic_model=topic_model)\n","    umass_final, npmi_final = calculate_umass_npmi(documents=documents_clean, topic_model=topic_model)\n","    topic_diversity_final = calculate_topic_diversity(topic_model=topic_model, top_n_words=10)\n","    topic_size_final = calculate_topic_size_imbalance(topics)\n","    \n","    return topic_model"]},{"cell_type":"code","execution_count":36,"id":"59c1cf94","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-03 12:33:24,615 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2025-05-03 12:33:43,719 - BERTopic - Dimensionality - Completed ✓\n","2025-05-03 12:33:43,720 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2025-05-03 12:33:44,419 - BERTopic - Cluster - Completed ✓\n","2025-05-03 12:33:44,423 - BERTopic - Representation - Fine-tuning topics using representation models.\n","2025-05-03 12:34:04,883 - BERTopic - Representation - Completed ✓\n"]},{"name":"stdout","output_type":"stream","text":["Topic Coherence (C_V): 0.5134\n","BERTopic U_Mass coherence  = -3.7624\n","BERTopic C_NPMI coherence  = -0.0140\n","BERTopic diversity = 0.6571  (184 unique of 280 total words)\n","BERTopic Topic size imbalance (max/min): 8.22\n","Number of topics (excluding noise): 28\n","0_party_republicans_democrats_biden:\n","party, republicans, democrats, biden, people, republican, change, like, rights, right\n","\n","1_nuclear_power_energy_nuclear power:\n","nuclear, power, energy, nuclear power, climate, germany, change, climate change, plants, gas\n","\n","2_florida_insurance_fraud_climate change:\n","florida, insurance, fraud, climate change, insurance companies, climate, change, hurricane, state, hurricanes\n","\n","3_cars_car_evs_ev:\n","cars, car, evs, ev, electric, vehicles, climate, change, energy, solar\n","\n","4_pakistan_https_die_floods:\n","pakistan, https, die, floods, india, com, climate, www, https www, country\n","\n"]}],"source":["# Apply pipeline to 'all-MiniLM-L6-v2' embedding\n","topic_model = pipeline(MiniLM_embeddings)\n","\n","# Print number of topics that BERT created\n","num_topics = len([t for t in topic_model.get_topics().keys() if t != -1])\n","print(f\"Number of topics (excluding noise): {num_topics}\")\n","\n","# Get topic info DataFrame\n","topic_info = topic_model.get_topic_info()\n","\n","# Exclude the noise topic (-1) and get top 5 by count\n","top_topics = topic_info[topic_info.Topic != -1].head(5)\n","\n","# Loop through top 5 topics\n","for _, row in top_topics.iterrows():\n","    topic_id = row[\"Topic\"]\n","    topic_name = row[\"Name\"]\n","    keywords = topic_model.get_topic(topic_id)\n","    \n","    print(f\"{topic_name}:\")\n","    print(\", \".join([word for word, _ in keywords]))\n","    print()"]},{"cell_type":"markdown","id":"a755c0fb","metadata":{},"source":["As the results show, the model identified 28 topics from the Reddit climate dataset (excluding noise). The topic coherence score (C_V = 0.5134) suggests that the topics are moderately interpretable, while the UMass and C_NPMI scores (–3.76 and –0.0140) indicate weaker word co-occurrence, which might be due to the informal and varied language used on Reddit. The topic diversity score of 0.6571 (184 unique out of 280 words) shows a fair amount of variation in topic descriptors, though some overlap between topics likely remains. The topic size imbalance of 8.22 means some topics are much larger than others, but the distribution is still manageable. Looking at the top topics, the model seems to have picked up on relevant themes such as political discussions, nuclear energy, insurance issues in Florida, electric vehicles, and climate-related events in Pakistan, showing that it can capture a broad range of climate-related conversations."]},{"cell_type":"markdown","id":"3f8a6f05","metadata":{},"source":["## Run BERTopic with MiniLM embeddings on 100,000 Comments"]},{"cell_type":"code","execution_count":6,"id":"8d6855a9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78ea2c39c71f453fa79622b6a4679c62","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/3077 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Select a subset of comments from the dataframe\n","documents = df.select(\"body\").limit(100000).rdd.flatMap(lambda x: x).collect()\n","\n","# Keep only posts with at least 5 meaningful words\n","documents_clean = [doc for doc in documents if len(doc.split()) >= 5]\n","\n","# Pre-calculate embeddings to avoid recalculating each time\n","embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n","MiniLM_embeddings = embedding_model.encode(documents_clean, show_progress_bar=True)"]},{"cell_type":"code","execution_count":7,"id":"49f7256a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Approx size: 0.09 GB\n"]}],"source":["# Calculate size of data used\n","total_size_bytes = sum(sys.getsizeof(doc) for doc in documents_clean)\n","size_gb = total_size_bytes / (1024 ** 3)\n","print(f\"Approx size: {size_gb:.2f} GB\")"]},{"cell_type":"code","execution_count":8,"id":"578e71f2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-03 14:29:22,472 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n","2025-05-03 14:31:44,419 - BERTopic - Dimensionality - Completed ✓\n","2025-05-03 14:31:44,423 - BERTopic - Cluster - Start clustering the reduced embeddings\n","2025-05-03 14:31:55,151 - BERTopic - Cluster - Completed ✓\n","2025-05-03 14:31:55,167 - BERTopic - Representation - Fine-tuning topics using representation models.\n","2025-05-03 14:33:34,827 - BERTopic - Representation - Completed ✓\n"]},{"name":"stdout","output_type":"stream","text":["Topic Coherence (C_V): 0.5463\n","BERTopic U_Mass coherence  = -6.1638\n","BERTopic C_NPMI coherence  = 0.0223\n","BERTopic diversity = 0.7381  (1299 unique of 1760 total words)\n","BERTopic Topic size imbalance (max/min): 50.30\n","CPU times: user 7min 40s, sys: 1min 21s, total: 9min 1s\n","Wall time: 5min 32s\n"]}],"source":["%%time\n","# Apply pipeline to 'all-MiniLM-L6-v2' embedding \n","topic_model = pipeline(MiniLM_embeddings)"]},{"cell_type":"code","execution_count":9,"id":"39ac6e6f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of topics (excluding noise): 176\n","0_climate change_change_climate_change climate:\n","climate change, change, climate, change climate, global warming, warming, global, warming climate, change global, solve climate\n","\n","1_peterson_climate change_climate_change:\n","peterson, climate change, climate, change, jordan, guy, just, like, jordan peterson, doesn\n","\n","2_meat_animal_vegan_animals:\n","meat, animal, vegan, animals, eat, eating, agriculture, veganism, food, diet\n","\n","3_kids_children_child_kid:\n","kids, children, child, kid, having, having kids, want, life, world, don\n","\n","4_labor_greens_australia_party:\n","labor, greens, australia, party, government, labour, uk, election, brexit, gt\n","\n"]}],"source":["# Print number of topics that BERT created\n","num_topics = len([t for t in topic_model.get_topics().keys() if t != -1])\n","print(f\"Number of topics (excluding noise): {num_topics}\")\n","\n","# Get topic info DataFrame\n","topic_info = topic_model.get_topic_info()\n","\n","# Exclude the noise topic (-1) and get top 5 by count\n","top_topics = topic_info[topic_info.Topic != -1].head(5)\n","\n","# Loop through top 5 topics\n","for _, row in top_topics.iterrows():\n","    topic_id = row[\"Topic\"]\n","    topic_name = row[\"Name\"]\n","    keywords = topic_model.get_topic(topic_id)\n","    \n","    print(f\"{topic_name}:\")\n","    print(\", \".join([word for word, _ in keywords]))\n","    print()"]},{"cell_type":"markdown","id":"ae1cb501","metadata":{},"source":["When scaling the BERTopic model to 100,000 Reddit comments, the number of identified topics increased substantially from 28 to 176, reflecting the broader thematic range present in the larger dataset. The topic coherence score improved slightly to 0.5463, indicating greater semantic clarity, while the topic diversity rose to 0.7381 (1299 unique of 1760 words), suggesting that the model captured a wide variety of distinct concepts. However, the topic size imbalance increased drastically to 50.30, implying that a few dominant topics absorbed a disproportionate number of documents—likely due to clustering challenges at this scale. UMass (–6.16) and C_NPMI (0.0223) coherence remained relatively low, consistent with prior findings and possibly reflecting the noisy, informal nature of Reddit language. Nevertheless, the top-ranked topics continue to be interpretable and thematically focused, covering themes such as climate change discourse, political figures like Jordan Peterson, diet and agriculture, family planning, and Australian political parties, indicating that the model remains effective at surfacing diverse climate-related narratives even at higher volumes."]},{"cell_type":"markdown","id":"bfafc44e","metadata":{},"source":["# BERTopic with 'all-MiniLM-L6-v2' embeddings and LLM (Groq llama3-70b-8192) representation model\n","As can be seen from the code above, the topic labels assigned by the applied BERTopic pipeline are simply a combination of the topics keywords, including reptitions of the same terms. To improve human interpretability and create more sound and clean topic labels, we prompt Groq llama3-70b-8192 via an API to provide us with topic labels."]},{"cell_type":"code","execution_count":29,"id":"d87e8482","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Topic 0: Climate Change Global Impact\n","Keywords: climate change, change, climate, change climate, global warming, warming, global, warming climate, change global, solve climate\n","\n","Topic 1: Jordan Peterson on Climate\n","Keywords: peterson, climate change, climate, change, jordan, guy, just, like, jordan peterson, doesn\n","\n","Topic 2: Veganism and Animal Agriculture\n","Keywords: meat, animal, vegan, animals, eat, eating, agriculture, veganism, food, diet\n","\n","Topic 3: Having Children in Life\n","Keywords: kids, children, child, kid, having, having kids, want, life, world, don\n","\n","Topic 4: Australian Labour Politics Brexit\n","Keywords: labor, greens, australia, party, government, labour, uk, election, brexit, gt\n","\n","Topic 5: COVID-19 Pandemic and Vaccines\n","Keywords: covid, vaccine, vaccines, diseases, virus, pandemic, deaths, people, disease, mask\n","\n","Topic 6: Christianity and Religious Beliefs\n","Keywords: god, religion, bible, religious, christian, christians, church, jesus, nbsp, amp nbsp\n","\n","Topic 7: Urban Transportation Modes\n","Keywords: cars, car, transit, bike, public, transport, rail, drive, public transit, traffic\n","\n","Topic 8: Human Extinction and Survival\n","Keywords: extinction, humans, species, extinct, humanity, earth, survive, human, die, mass\n","\n","Topic 9: Corporate Carbon Responsibility Debates\n","Keywords: corporations, companies, consumers, responsible, carbon, individual, individuals, footprint, blame, responsibility\n","\n"]}],"source":["# Set Groq API Key\n","GROQ_API_KEY = \"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\"\n","client = groq.Groq(api_key=GROQ_API_KEY)\n","\n","# Define prompt template\n","prompt_template = \"\"\"\n","I have a topic that contains the following documents:\n","{documents}\n","\n","The topic is described by the following keywords: {keywords}\n","\n","Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n","topic: <topic label>\n","\"\"\"\n","\n","# Label cleaning function\n","def get_groq_label(documents, keywords):\n","    prompt = prompt_template.format(\n","        documents=documents,\n","        keywords=\", \".join(keywords)\n","    )\n","    response = client.chat.completions.create(\n","        model=\"llama3-70b-8192\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    )\n","    raw_text = response.choices[0].message.content.strip()\n","    match = re.search(r\"topic:\\s*(.+)\", raw_text, re.IGNORECASE)\n","    return match.group(1).strip() if match else raw_text\n","\n","# Get top 10 topics (excluding noise)\n","topic_info = topic_model.get_topic_info()\n","top_10 = topic_info[topic_info.Topic != -1].head(10)\n","\n","# Generate and print labels\n","for _, row in top_10.iterrows():\n","    topic_id = row[\"Topic\"]\n","    keywords = [word for word, _ in topic_model.get_topic(topic_id)[:10]]\n","    label = get_groq_label([\" \".join(keywords)], keywords)\n","\n","    print(f\"Topic {topic_id}: {label}\")\n","    print(\"Keywords:\", \", \".join(keywords))\n","    print()\n"]},{"cell_type":"markdown","id":"bb22f5ef","metadata":{},"source":["As the output shows, Groq generates highly interpretable topic labels that align well with the underlying keywords. This makes it a valuable extension to the BERTopic pipeline, enhancing the overall clarity and usability of the resulting topic model."]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}