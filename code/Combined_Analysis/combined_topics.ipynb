{"cells":[{"cell_type":"markdown","id":"ce3ac7bb","metadata":{},"source":["# Understanding Climate Change Discourse on Reddit: A Distributed Analysis of Public Themes, Sentiment, and Recommendations"]},{"cell_type":"markdown","id":"77950fd0","metadata":{},"source":["### Candidate numbers: 39884, 48099, 49308, 50250"]},{"cell_type":"markdown","id":"c28f2f82","metadata":{},"source":["## Notebook Overview\n","\n","This notebook conducts topic modeling on the dataset using Latent Dirichlet Allocation (LDA) algorithm. This was the chosen topic modeling technique due to the performance and more importantly the interpretability. \n","\n","The goal is to uncover dominant themes, identify how these topics evolve across time, and visualize meaningful patterns in public discourse. The first step of this is to predict the topic each of the comments belong to. To improve human interpretability and create more sound and clean topic labels, we prompt Groq llama3-70b-8192 via an API to provide us with topic labels. The predicted topic and topic name is then saved in a column in the main dataframe.\n","\n","The main dataframe `df_tf` is then saved as a parquet file in a bucket so that it can be imported for the sentiment analysis and visualisation part of this section."]},{"cell_type":"markdown","id":"6bc7448a","metadata":{},"source":["## Cluster Setup and Initialization Actions\n","\n","We used Google Cloud Dataproc to create a scalable cluster with the following settings:\n","\n","#### Create the bucket\n","```gsutil mb gs://st446-gp-sm```\n","\n","#### Upload the initialization script\n","```gsutil cp my_actions.sh gs://st446-gp-sm```\n","\n","#### Create the Dataproc cluster\n","```gcloud dataproc clusters create st446-cluster-gp-sm \\\n","  --enable-component-gateway \\\n","  --public-ip-address \\\n","  --region europe-west1 \\\n","  --bucket=st446-gp-sm \\\n","  --master-machine-type n2-standard-2 \\\n","  --master-boot-disk-size 100 \\\n","  --num-workers 2 \\\n","  --worker-machine-type n2-standard-2 \\\n","  --worker-boot-disk-size 200 \\\n","  --image-version 2.2-debian12 \\\n","  --optional-components JUPYTER \\\n","  --metadata 'PIP_PACKAGES=sklearn nltk pandas numpy' \\\n","  --initialization-actions 'gs://st446-gp-sm/my_actions.sh' \\\n","  --project=capstone-data-1-wto```"]},{"cell_type":"code","execution_count":1,"id":"7f589c33","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/default/lib/python3.11/site-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/default/lib/python3.11/site-packages (from gensim) (1.11.4)\n","Collecting smart-open>=1.8.1 (from gensim)\n","  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n","Collecting wrapt (from smart-open>=1.8.1->gensim)\n","  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m156.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n","Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n","Installing collected packages: wrapt, smart-open, gensim\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3/3\u001B[0m [gensim]2m2/3\u001B[0m [gensim]\n","\u001B[1A\u001B[2KSuccessfully installed gensim-4.3.3 smart-open-7.1.0 wrapt-1.17.2\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0mCollecting groq\n","  Downloading groq-0.24.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/default/lib/python3.11/site-packages (from groq) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/default/lib/python3.11/site-packages (from groq) (1.8.0)\n","Collecting httpx<1,>=0.23.0 (from groq)\n","  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/default/lib/python3.11/site-packages (from groq) (2.11.3)\n","Requirement already satisfied: sniffio in /opt/conda/default/lib/python3.11/site-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/conda/default/lib/python3.11/site-packages (from groq) (4.13.2)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/default/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n","Requirement already satisfied: certifi in /opt/conda/default/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n","  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n","Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n","  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/default/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/default/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/default/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n","Downloading groq-0.24.0-py3-none-any.whl (127 kB)\n","Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n","Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n","Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n","Installing collected packages: h11, httpcore, httpx, groq\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4/4\u001B[0m [groq][32m3/4\u001B[0m [groq]\n","\u001B[1A\u001B[2KSuccessfully installed groq-0.24.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["# Import libraries used in this notebook\n","import zipfile\n","!pip install gensim\n","!pip install groq\n","import os\n","import re\n","import hashlib\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","import string\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import string\n","from pyspark.sql import SparkSession\n","import pyspark.sql.functions as sql_f \n","from pyspark.ml.feature import CountVectorizer\n","from pyspark.ml.clustering import LDA\n","from time import time\n","from pyspark.sql.functions import udf, col, rand, monotonically_increasing_id\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n","from pyspark.ml.feature import StopWordsRemover, Tokenizer, CountVectorizer, IDF\n","from pyspark.sql.functions import lower, regexp_replace, row_number, desc\n","import random\n","from pyspark.sql.functions import rand\n","from gensim.corpora import Dictionary\n","from gensim.models.coherencemodel import CoherenceModel\n","from pyspark.sql.functions import year\n","from pyspark.sql.window import Window\n","import matplotlib.pyplot as plt\n","import groq\n","from pyspark.ml.classification import LogisticRegression"]},{"cell_type":"markdown","id":"f9dcd6f0","metadata":{},"source":["# Data Loading Preprocessing and Cleaning"]},{"cell_type":"code","execution_count":2,"id":"bbc6bf0d","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 1536M  100 1536M    0     0  40.1M      0  0:00:38  0:00:38 --:--:-- 41.2M\n","Archive:  climate.zip\n","  inflating: the-reddit-climate-change-dataset-comments.csv  \n","  inflating: the-reddit-climate-change-dataset-posts.csv  \n","Found 4 items\n","-rw-r--r--   2 root hadoop 4111000325 2025-05-05 08:49 /the-reddit-climate-change-dataset-comments.csv\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-05 08:43 /tmp\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-05 08:46 /user\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-05 08:41 /var\n"]}],"source":["# Download the Kaggle dataset zip\n","!curl -L -o climate.zip \\\n","    \"https://www.kaggle.com/api/v1/datasets/download/pavellexyr/the-reddit-climate-change-dataset\"\n","\n","# Unzip it (this will extract all files, including the comments CSV)\n","!unzip -o climate.zip\n","\n","# Remove any old copy in HDFS and put the comments file there\n","!hadoop fs -rm -f /the-reddit-climate-change-dataset-comments.csv\n","!hadoop fs -put the-reddit-climate-change-dataset-comments.csv /\n","\n","# Verify upload\n","!hadoop fs -ls /"]},{"cell_type":"code","execution_count":3,"id":"3d5a8e1b","metadata":{"tags":[]},"outputs":[],"source":["# Point to the new HDFS path (edit according to your cluster name)\n","comments_path = \"hdfs://st446-cluster-gp-sm-m:8020/the-reddit-climate-change-dataset-comments.csv\""]},{"cell_type":"code","execution_count":4,"id":"112c4673","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- type: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- subreddit.id: string (nullable = true)\n"," |-- subreddit.name: string (nullable = true)\n"," |-- subreddit.nsfw: string (nullable = true)\n"," |-- created_utc: string (nullable = true)\n"," |-- permalink: string (nullable = true)\n"," |-- body: string (nullable = true)\n"," |-- sentiment: double (nullable = true)\n"," |-- score: integer (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+-------+------------+-----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","|   type|     id|subreddit.id|   subreddit.name|subreddit.nsfw|created_utc|           permalink|                body|sentiment|score|\n","+-------+-------+------------+-----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","|comment|hv6kbym|       2qh4r|       conspiracy|         false| 1643748232|https://old.reddi...|What they’ll prob...|    -0.34|    2|\n","|comment|dz5bszu|       2qh3l|             news|         false| 1526585977|https://old.reddi...|I think climate c...|   0.8113|    1|\n","|comment|fm20ywf|       2qhsa|interestingasfuck|         false| 1585688617|https://old.reddi...|You're right that...|   0.9311|    2|\n","|comment|h10axzg|       vkedk|         themotte|         false| 1623138758|https://old.reddi...|Epistemic status:...|  -0.9927|   53|\n","|comment|f29z91o|       2qh1s|        economics|         false| 1570106360|https://old.reddi...|Prices seem right...|   0.9939|    4|\n","+-------+-------+------------+-----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","only showing top 5 rows\n","\n"]}],"source":["# Define the schema for the comments file\n","schema = StructType([\n","    StructField(\"type\",           StringType(), True),\n","    StructField(\"id\",             StringType(), True),\n","    StructField(\"subreddit.id\",   StringType(), True),\n","    StructField(\"subreddit.name\", StringType(), True),\n","    StructField(\"subreddit.nsfw\", StringType(), True),\n","    StructField(\"created_utc\",    StringType(), True),\n","    StructField(\"permalink\",      StringType(), True),\n","    StructField(\"body\",           StringType(), True),\n","    StructField(\"sentiment\",      DoubleType(), True),\n","    StructField(\"score\",          IntegerType(),True)\n","])\n","\n","df = spark.read \\\n","    .option(\"header\", \"true\") \\\n","    .option(\"multiLine\", \"true\") \\\n","    .option(\"escape\", \"\\\"\") \\\n","    .schema(schema) \\\n","    .csv(comments_path)\n","\n","df = df.repartition(8)\n","df.printSchema()\n","df.show(5)"]},{"cell_type":"code","execution_count":5,"id":"95a8a13b","metadata":{"tags":[]},"outputs":[],"source":["custom_stopwords = set([\n","    \"lt\", \"gt\", \"ref\", \"quot\", \"cite\", \"br\", \"amp\", \"https\", \"http\", \"urlhttps\", \"urlhttp\", \n","    \"file\", \"image\", \"jpg\", \"png\", \"gif\", \"svg\", \"thumb\", \"px\", \"category\", \"url\", \"external\", \n","    \"link\", \"source\", \"web\", \"cite\", \"reference\", \"reflist\", \"main\", \"article\", \"seealso\", \n","    \"further\", \"infobox\", \"template\", \"navbox\", \"redirect\", \"harvnb\", \"isbn\", \"doi\", \"pmid\", \n","    \"ssrn\", \"jstor\", \"bibcode\", \"arxiv\", \"ol\", \"hdl\", \"wikidata\", \"wiki\", \"math\", \"sup\", \"sub\", \n","    \"nbsp\", \"equation\", \"displaystyle\", \"begin\", \"end\", \"left\", \"right\", \"sqrt\", \"frac\", \"sum\", \n","    \"prod\", \"int\", \"lim\", \"rightarrow\", \"infty\", \"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \n","    \"zeta\", \"eta\", \"theta\", \"iota\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"xi\", \"omicron\", \"pi\", \"rho\", \n","    \"sigma\", \"tau\", \"upsilon\", \"phi\", \"chi\", \"psi\", \"omega\", \"mathrm\", \"mathbb\", \"mathcal\", \n","    \"mathbf\", \"cdots\", \"ldots\", \"vdots\", \"ddots\", \"forall\", \"exists\", \"in\", \"ni\", \"subset\", \n","    \"subseteq\", \"supset\", \"supseteq\", \"emptyset\", \"cap\", \"cup\", \"setminus\", \"not\", \"times\", \n","    \"div\", \"cdot\", \"pm\", \"mp\", \"oplus\", \"otimes\", \"odot\", \"leq\", \"geq\", \"neq\", \"approx\", \n","    \"aligncenter\", \"fontsize\", \"alignright\", \"alignleft\", \"textalign\", \"bold\", \"italic\", \n","    \"underline\", \"strikethrough\", \"lineheight\", \"padding\", \"margin\", \"width\", \"height\", \"float\", \n","    \"clear\", \"border\", \"background\", \"color\", \"font\", \"family\", \"size\", \"weight\", \"style\", \n","    \"decoration\", \"verticalalign\", \"textindent\", \"pre-wrap\", \"nowrap\", \"valign\", \"bgcolor\", \n","    \"style\", \"class\", \"id\", \"width\", \"height\", \"align\", \"border\", \"cellpadding\", \"cellspacing\", \n","    \"colspan\", \"rowspan\", \"nowrap\", \"target\", \"rel\", \"hreflang\", \"title\", \"alt\", \"src\", \"dir\", \n","    \"lang\", \"type\", \"name\", \"value\", \"readonly\", \"multiple\", \"onclick\", \"onmousedown\", \n","    \"onmouseup\", \"onmouseover\", \"onmouseout\", \"onload\", \"onunload\", \"onsubmit\", \"onreset\", \n","    \"onfocus\", \"onblur\", \"onkeydown\", \"onkeyup\", \"onkeypress\", \"onerror\", \"infobox\", \"caption\", \n","    \"cite\", \"dmy\", \"mdy\", \"date\", \"archive\", \"www\", \"com\", \"org\", \"access\", \"ndash\", \"sfn\", \"dts\", \"vauthors\", \n","    \"mvar\", \"ipaslink\", \"ipa\", \"iii\", \"ibn\", \"first\", \"last\", \"also\", \"html\", \"use\", \"publisher\", \"year\", \"one\", \n","    \"page\", \"new\", \"trek\", \"ipablink\", \"similar\", \"usual\", \"two\", \"abbr\", \"used\", \"est\", \"ibm\", \"first1\",\n","    \"first2\", \"last1\", \"last2\", \"pdf\", \"der\", \"ted\", \"get\", \"even\", \"isn\", \"going\", \"like\", \"people\", \"even\",\n","    \"still\", \"doesn\", \"much\", \"make\", \"many\", \"made\", \"don\", \"did\"\n","])"]},{"cell_type":"code","execution_count":6,"id":"cbf6a619","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Sample size: 920435 comments\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 11:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|          body_clean|        final_tokens|\n","+--------------------+--------------------+\n","|warren is fine an...|[warren, fine, ec...|\n","| gt just not enth...|[enthusiastic, cl...|\n","|first of all we n...|[need, rid, prete...|\n","| gt each one of y...|[points, clearly,...|\n","| gt debating cont...|[debating, conten...|\n","+--------------------+--------------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Draw a 20% sample of the raw comments (due to memory constraints)\n","sample_df = df.sample(withReplacement=False, fraction=0.20, seed=42)\n","print(f\"Sample size: {sample_df.count()} comments\")\n","\n","# Clean text on the sample, remove symbols, links and make everything lower case\n","df_clean = (\n","    sample_df\n","      .withColumn(\"body_clean\", lower(col(\"body\")))\n","      .withColumn(\"body_clean\", regexp_replace(\"body_clean\", r\"http\\S+\", \"\"))  \n","      .withColumn(\"body_clean\", regexp_replace(\"body_clean\", r\"[^a-z\\s]\", \" \"))  \n","      .withColumn(\"body_clean\", regexp_replace(\"body_clean\", r\"\\s+\", \" \"))     \n",")\n","\n","# Tokenize\n","tokenizer = Tokenizer(inputCol=\"body_clean\", outputCol=\"tokens\")\n","df_tokens = tokenizer.transform(df_clean)\n","\n","# Remove stop‐words (based on general and custom list)\n","default_stops = StopWordsRemover.loadDefaultStopWords(\"english\")\n","combined_stops = list(set(default_stops) | custom_stopwords)\n","\n","remover = StopWordsRemover(\n","    inputCol=\"tokens\",\n","    outputCol=\"filtered\",\n","    stopWords=combined_stops\n",")\n","df_no_stop = remover.transform(df_tokens)\n","\n","# Filter out very short tokens (words of 1 or 2 characters like 'a' and 'is')\n","min_len = udf(lambda toks: [t for t in toks if len(t) > 2], ArrayType(StringType()))\n","df_final = df_no_stop.withColumn(\"final_tokens\", min_len(col(\"filtered\")))\n","\n","df_final.select(\"body_clean\", \"final_tokens\").show(5)"]},{"cell_type":"code","execution_count":7,"id":"913a7f20","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Vectorize on the sample to get the features\n","cv = CountVectorizer(\n","    inputCol=\"final_tokens\",\n","    outputCol=\"rawFeatures\",\n","    minDF=50,\n","    maxDF=0.8\n",")\n","cv_model = cv.fit(df_final)\n","df_tf = cv_model.transform(df_final)"]},{"cell_type":"code","execution_count":8,"id":"6a57ea8c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|        final_tokens|         rawFeatures|\n","+--------------------+--------------------+\n","|[warren, fine, ec...|(26883,[2,5,11,18...|\n","|[enthusiastic, cl...|(26883,[11,20,25,...|\n","|[need, rid, prete...|(26883,[4,6,7,12,...|\n","|[points, clearly,...|(26883,[1,7,10,22...|\n","|[debating, conten...|(26883,[12,39,48,...|\n","+--------------------+--------------------+\n","only showing top 5 rows\n","\n","Sample vocab size: 26883\n","First 20 sample-vocab entries: ['think', 'world', 'years', 'time', 'know', 'way', 'need', 'want', 'global', 'things', 'really', 'see', 'say', 'good', 'well', 'science', 'believe', 'point', 'trump', 'actually']\n"]}],"source":["df_tf.select(\"final_tokens\", \"rawFeatures\").show(5)\n","\n","vocab_sample = cv_model.vocabulary\n","print(f\"Sample vocab size: {len(vocab_sample)}\")\n","print(\"First 20 sample-vocab entries:\", vocab_sample[:20])"]},{"cell_type":"markdown","id":"8a4c350e","metadata":{},"source":["# Topic Modelling"]},{"cell_type":"markdown","id":"367904fd","metadata":{},"source":["## LDA"]},{"cell_type":"markdown","id":"773efc0b","metadata":{},"source":["We use PySpark's MLlib `LDA` class to identify latent topics. The dataset is tokenized, stopwords are removed, and a CountVectorizer is applied to prepare term-frequency vectors suitable for LDA input. The model is trained with a 10 topics."]},{"cell_type":"code","execution_count":9,"id":"eca110c0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Train LDA\n","k = 10\n","max_iters = 20\n","\n","lda = LDA(\n","    k=k,\n","    maxIter=max_iters,\n","    featuresCol=\"rawFeatures\",\n","    seed=123,          \n",")\n","\n","# Fit on vectorized data\n","lda_model = lda.fit(df_tf)"]},{"cell_type":"code","execution_count":10,"id":"4b74c87b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+-----------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|topic|termIndices                                    |termWeights                                                                                                                                                                                                                       |\n","+-----+-----------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|0    |[188, 249, 20, 18, 27, 463, 108, 436, 22, 201] |[0.007001323950551166, 0.005934168170880602, 0.0051651659933804415, 0.004944025210571986, 0.004637482641750501, 0.004553339558191248, 0.004137928781719031, 0.004075382501376867, 0.003542029869714262, 0.0034017975175688504]    |\n","|1    |[18, 0, 281, 2, 20, 1, 16, 3, 155, 4]          |[0.010885420416691062, 0.0046947362186161076, 0.0034570694407365854, 0.003150712622645226, 0.003125203342921963, 0.0029777719871057675, 0.002962689681524247, 0.0028062138711507987, 0.002776879103589657, 0.002743802429291886]  |\n","|2    |[15, 4, 0, 23, 67, 9, 1, 5, 12, 116]           |[0.009700384509037763, 0.007236561045851404, 0.006275859035959595, 0.005097892041731437, 0.004506096859455637, 0.004379748907725804, 0.0042530586202271055, 0.004157312202888469, 0.004091826686877241, 0.003930740533746866]     |\n","|3    |[29, 8, 2, 40, 195, 225, 132, 32, 3, 57]       |[0.01031397190217284, 0.009977425579890635, 0.008082651147796356, 0.0072286242281178505, 0.00627911680057091, 0.0057706111080559515, 0.004700468420919071, 0.004456616811429658, 0.004224721890603645, 0.003769822471536969]      |\n","|4    |[3, 1, 10, 0, 13, 5, 9, 14, 37, 28]            |[0.005200609747510873, 0.004047826726600608, 0.003983221860347354, 0.0035866592252706375, 0.0034443740855893985, 0.0034396867975070144, 0.0033139327161778136, 0.0031842015817577685, 0.003155174306079711, 0.003014220035065685] |\n","|5    |[0, 73, 7, 27, 133, 44, 9, 18, 12, 5]          |[0.0061826717401338585, 0.005443583586389785, 0.004494986833328513, 0.004028688597690003, 0.004000676877750469, 0.0039973978208254005, 0.0036372161328589087, 0.0035314091028456874, 0.0031880684085349048, 0.0031433660697976077]|\n","|6    |[0, 1, 16, 4, 2, 10, 5, 3, 9, 12]              |[0.01282620649438985, 0.006918647968667194, 0.0061418711001351755, 0.005826054085869464, 0.0056625269972681675, 0.005394877140659073, 0.005078062533679509, 0.004993409106323031, 0.004792016481237972, 0.004680186201321138]     |\n","|7    |[74, 134, 22, 35, 244, 488, 241, 937, 383, 140]|[0.03339581121436479, 0.0256981824184219, 0.018264628065310602, 0.01646101919829566, 0.0083843034220179, 0.007681408192030132, 0.006865052854096907, 0.0059853052756865515, 0.005633459385673132, 0.004880294695787987]           |\n","|8    |[65, 1, 8, 138, 149, 79, 2, 95, 139, 34]       |[0.009668661254208583, 0.00927702267803055, 0.006098426342832925, 0.005509458825967468, 0.005432864459259206, 0.0042376324863045755, 0.0037586894951317507, 0.0036314655748731287, 0.003443981109214672, 0.003286126911199618]    |\n","|9    |[32, 33, 57, 0, 6, 22, 84, 1, 166, 5]          |[0.007098766576348012, 0.005895565991744328, 0.005519585905312551, 0.0053939167572575625, 0.005310019580827735, 0.0051715697241049765, 0.004124096811815537, 0.0038093524584694614, 0.003792108461789057, 0.0037596380621575235]  |\n","+-----+-----------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 130:==========================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["\n","Identified Topics:\n","Topic 0:\n","  bernie: 0.0070\n","  national: 0.0059\n","  said: 0.0052\n","  trump: 0.0049\n","  government: 0.0046\n","  bot: 0.0046\n","  china: 0.0041\n","  security: 0.0041\n","  energy: 0.0035\n","  states: 0.0034\n","\n","Topic 1:\n","  trump: 0.0109\n","  think: 0.0047\n","  white: 0.0035\n","  years: 0.0032\n","  said: 0.0031\n","  world: 0.0030\n","  believe: 0.0030\n","  time: 0.0028\n","  anti: 0.0028\n","  know: 0.0027\n","\n","Topic 2:\n","  science: 0.0097\n","  know: 0.0072\n","  think: 0.0063\n","  real: 0.0051\n","  scientists: 0.0045\n","  things: 0.0044\n","  world: 0.0043\n","  way: 0.0042\n","  say: 0.0041\n","  scientific: 0.0039\n","\n","Topic 3:\n","  warming: 0.0103\n","  global: 0.0100\n","  years: 0.0081\n","  earth: 0.0072\n","  ice: 0.0063\n","  temperature: 0.0058\n","  data: 0.0047\n","  carbon: 0.0045\n","  time: 0.0042\n","  emissions: 0.0038\n","\n","Topic 4:\n","  time: 0.0052\n","  world: 0.0040\n","  really: 0.0040\n","  think: 0.0036\n","  good: 0.0034\n","  way: 0.0034\n","  things: 0.0033\n","  well: 0.0032\n","  work: 0.0032\n","  lot: 0.0030\n","\n","Topic 5:\n","  think: 0.0062\n","  party: 0.0054\n","  want: 0.0045\n","  government: 0.0040\n","  vote: 0.0040\n","  political: 0.0040\n","  things: 0.0036\n","  trump: 0.0035\n","  say: 0.0032\n","  way: 0.0031\n","\n","Topic 6:\n","  think: 0.0128\n","  world: 0.0069\n","  believe: 0.0061\n","  know: 0.0058\n","  years: 0.0057\n","  really: 0.0054\n","  way: 0.0051\n","  time: 0.0050\n","  things: 0.0048\n","  say: 0.0047\n","\n","Topic 7:\n","  nuclear: 0.0334\n","  biden: 0.0257\n","  energy: 0.0183\n","  power: 0.0165\n","  solar: 0.0084\n","  wind: 0.0077\n","  coal: 0.0069\n","  joe: 0.0060\n","  plants: 0.0056\n","  fossil: 0.0049\n","\n","Topic 8:\n","  water: 0.0097\n","  world: 0.0093\n","  global: 0.0061\n","  food: 0.0055\n","  population: 0.0054\n","  countries: 0.0042\n","  years: 0.0038\n","  due: 0.0036\n","  war: 0.0034\n","  human: 0.0033\n","\n","Topic 9:\n","  carbon: 0.0071\n","  money: 0.0059\n","  emissions: 0.0055\n","  think: 0.0054\n","  need: 0.0053\n","  energy: 0.0052\n","  oil: 0.0041\n","  world: 0.0038\n","  companies: 0.0038\n","  way: 0.0038\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Describe the top 6 terms for each of the k=10 topics\n","topics = lda_model.describeTopics(10)\n","topics.show(truncate=False)\n","\n","# Retrieve vocabulary from the CountVectorizer\n","vocab = cv_model.vocabulary\n","\n","# Map each topic to its actual words and weights\n","topic_words = topics.rdd.map(\n","    lambda row: (\n","        row['topic'],\n","        [vocab[i] for i in row['termIndices']],\n","        row['termWeights']\n","    )\n",").collect()\n","\n","print(\"\\nIdentified Topics:\")\n","for topic, words, weights in topic_words:\n","    print(f\"Topic {topic}:\")\n","    for w, wt in zip(words, weights):\n","        print(f\"  {w}: {wt:.4f}\")\n","    print()"]},{"cell_type":"markdown","id":"de465def","metadata":{},"source":["## LLM (Groq llama3-70b-8192) representation model\n","To improve human interpretability and create more sound and clean topic labels, we prompt Groq llama3-70b-8192 via an API to provide us with topic labels."]},{"cell_type":"code","execution_count":11,"id":"d1008089","metadata":{},"outputs":[],"source":["keywords_per_topic = {topic: words for topic, words, _ in topic_words}"]},{"cell_type":"code","execution_count":12,"id":"ced9ebbe","metadata":{},"outputs":[],"source":["import groq\n","\n","# Set Groq API Key\n","GROQ_API_KEY = \"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\"\n","client = groq.Groq(api_key=GROQ_API_KEY)\n","\n","# Define prompt template\n","prompt_template = \"\"\"\n","I have a topic described by the following keywords:\n","{keywords}\n","\n","Based on these keywords, generate a short and descriptive topic label of at most 5 words.\n","Make sure the output follows this format:\n","topic: <topic label>\n","\"\"\""]},{"cell_type":"code","execution_count":13,"id":"3ef2a31a","metadata":{},"outputs":[],"source":["# Label function\n","def get_groq_label(keywords):\n","    prompt = prompt_template.format(keywords=\", \".join(keywords))\n","    response = client.chat.completions.create(\n","        model=\"llama3-70b-8192\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    )\n","    raw_text = response.choices[0].message.content.strip()\n","    match = re.search(r\"topic:\\s*(.+)\", raw_text, re.IGNORECASE)\n","    return match.group(1).strip() if match else raw_text"]},{"cell_type":"code","execution_count":14,"id":"4d8286f9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== GROQ Labeled Topics ===\n","Topic 0: US-China National Security\n","Keywords: bernie, national, said, trump, government, bot, china, security, energy, states\n","\n","Topic 1: Trump's Anti-World Beliefs\n","Keywords: trump, think, white, years, said, world, believe, time, anti, know\n","\n","Topic 2: How Scientists View the World\n","Keywords: science, know, think, real, scientists, things, world, way, say, scientific\n","\n","Topic 3: Global Warming Trends\n","Keywords: warming, global, years, earth, ice, temperature, data, carbon, time, emissions\n","\n","Topic 4: Thinking the World Works\n","Keywords: time, world, really, think, good, way, things, well, work, lot\n","\n","Topic 5: Expressing Political Party Views\n","Keywords: think, party, want, government, vote, political, things, trump, say, way\n","\n","Topic 6: Understanding the World's Reality\n","Keywords: think, world, believe, know, years, really, way, time, things, say\n","\n","Topic 7: Biden's Nuclear Energy Shift\n","Keywords: nuclear, biden, energy, power, solar, wind, coal, joe, plants, fossil\n","\n","Topic 8: Global Water War Ahead\n","Keywords: water, world, global, food, population, countries, years, due, war, human\n","\n","Topic 9: Rethinking Energy Company Profits\n","Keywords: carbon, money, emissions, think, need, energy, oil, world, companies, way\n","\n"]}],"source":["lda_topic_labels = {}\n","\n","print(\"=== GROQ Labeled Topics ===\")\n","for topic_id, words, _ in topic_words:\n","    label = get_groq_label(words[:10])\n","    lda_topic_labels[topic_id] = label\n","    print(f\"Topic {topic_id}: {label}\")\n","    print(\"Keywords:\", \", \".join(words[:10]))\n","    print()"]},{"cell_type":"markdown","id":"89e65e55","metadata":{},"source":["## Comment on LDA Topics and LLM Labeling\n","\n","As can be seen above the LDA model has effectively grouped thematically consistent comments, and the LLM-based labels give human-readable topics to improve interpretability.\n","\n","Notably:\n","- **Topics 0, 1, and 7** are clearly tied to political and energy policy debates (e.g., US-China security, Trump’s worldview, Biden’s nuclear stance).\n","- **Topics 3 and 8** align with global environmental concerns (climate change, water scarcity).\n","- **Topics 2, 4, and 6** reflect philosophical discussion around science and human understanding of the world.\n","- **Topic 5** captures political party alignment discussions."]},{"cell_type":"markdown","id":"b2dab8b0","metadata":{},"source":["## Predict topic for each document"]},{"cell_type":"code","execution_count":15,"id":"0524185c","metadata":{},"outputs":[],"source":["# To predict for each comment in the dataframe\n","df_tf = lda_model.transform(df_tf)\n","\n","# UDF to extract dominant topic\n","def dominant_topic(topicDist):\n","    return int(topicDist.argmax())\n","\n","dominant_topic_udf = udf(dominant_topic, IntegerType())\n","\n","df_tf = df_tf.withColumn(\"predictedTopic\", dominant_topic_udf(\"topicDistribution\"))\n","\n","# topic: (words, weights) dictionary\n","topics50 = lda_model.describeTopics(50)\n","vocab = cv_model.vocabulary\n","\n","topic_full = topics50.rdd.map(lambda row: (\n","    row['topic'],\n","    [vocab[i] for i in row['termIndices']],\n","    row['termWeights']\n",")).collect()\n","\n","topic_full_dict = {t: (words, weights) for t, words, weights in topic_full}\n","topic_full_b = sc.broadcast(topic_full_dict)\n","\n","# UDF to compute top words per comment\n","def doc_top_words(td, raw, num=10):\n","    dom = int(td.argmax())\n","    if dom not in topic_full_b.value:\n","        return []\n","    cand_words, cand_wts = topic_full_b.value[dom]\n","\n","    counts = {i: v for i, v in zip(raw.indices, raw.values)} if raw else {}\n","\n","    dom_prob = float(td[dom])\n","    scores = {}\n","    for w, wt in zip(cand_words, cand_wts):\n","        try:\n","            idx = vocab.index(w)\n","            freq = counts.get(idx, 0)\n","        except ValueError:\n","            freq = 0\n","        noise = 1 + 0.05 * random.random()\n","        scores[w] = freq * wt * dom_prob * noise\n","\n","    sorted_ = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n","    if all(s == 0 for _, s in sorted_):\n","        return random.sample(cand_words, min(num, len(cand_words)))\n","    return [w for w, _ in sorted_[:num]]\n","\n","doc_top_udf = udf(doc_top_words, ArrayType(StringType()))\n","\n","# Obtain top Words in document\n","df_tf = df_tf.withColumn(\"docTopWords\", doc_top_udf(\"topicDistribution\", \"rawFeatures\"))"]},{"cell_type":"code","execution_count":16,"id":"a722dece","metadata":{},"outputs":[],"source":["topic_label_b = sc.broadcast(lda_topic_labels)\n","\n","# UDF to get the topic label using predictedTopic\n","def map_topic_label(topic_id):\n","    return topic_label_b.value.get(topic_id, \"Unknown\")\n","\n","topic_label_udf = udf(map_topic_label, StringType())\n","\n","df_tf = df_tf.withColumn(\"PredictedTopicName\", topic_label_udf(\"predictedTopic\"))"]},{"cell_type":"code","execution_count":18,"id":"3c9ac562","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 09:14:47 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n","[Stage 134:==========================================>              (6 + 2) / 8]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------+--------------+--------------------+--------------------+\n","|          body_clean|     subreddit|predictedTopic|         docTopWords|  PredictedTopicName|\n","+--------------------+--------------+--------------+--------------------+--------------------+\n","|both and brics ma...|    conspiracy|             0|[china, global, p...|US-China National...|\n","|personally i thin...|         meirl|             8|[water, world, th...|Global Water War ...|\n","|i m a geologist b...|      politics|             3|[earth, think, st...|Global Warming Tr...|\n","|it gets trickier ...|   libertarian|             5|[party, every, th...|Expressing Politi...|\n","|i think it is tot...|    truereddit|             2|[think, believe, ...|How Scientists Vi...|\n","|are you really no...|     dankmemes|             2|[really, well, sh...|How Scientists Vi...|\n","|looks like all th...| unitedkingdom|             4|[good, thing, day...|Thinking the Worl...|\n","|i think the key w...|onguardforthee|             9|[think, oil, bett...|Rethinking Energy...|\n","|what the heck doe...|      politics|             5|[issues, want, ta...|Expressing Politi...|\n","|what relevance do...|    badscience|             2|[science, scienti...|How Scientists Vi...|\n","|my point is clima...|  changemyview|             2|[point, want, sci...|How Scientists Vi...|\n","|talk about enviro...|     newjersey|             0|[state, environme...|US-China National...|\n","|insurance compani...|      business|             9|[less, companies,...|Rethinking Energy...|\n","|let me try my bes...|    futurology|             9|[industry, think,...|Rethinking Energy...|\n","|google means infi...|   singularity|             4|[time, lot, part,...|Thinking the Worl...|\n","+--------------------+--------------+--------------+--------------------+--------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 134:=================================================>       (7 + 1) / 8]\r","\r","                                                                                \r"]}],"source":["result_lda = df_tf.selectExpr(\n","    \"body_clean\",\n","    \"`subreddit.name` AS subreddit\",\n","    \"predictedTopic\",\n","    \"docTopWords\",\n","    \"PredictedTopicName\"\n",").orderBy(rand()).limit(15)\n","\n","result_lda.show()"]},{"cell_type":"markdown","id":"37a1a83d","metadata":{},"source":["# Saving df_tf"]},{"cell_type":"code","execution_count":19,"id":"a7406e4f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 09:22:33 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n","                                                                                \r"]}],"source":["# Define your output path (change bucket name accordingly)\n","output_path = \"gs://st446-gp-sm/processed_data/df_tf_after_topics\"\n","\n","# Save DataFrame as Parquet\n","df_tf.write.mode(\"overwrite\").parquet(output_path)"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}