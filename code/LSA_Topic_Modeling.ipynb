{"cells":[{"cell_type":"markdown","id":"c6486d95","metadata":{},"source":["# Understanding Climate Change Discourse on Reddit: A Distributed Analysis of Public Themes, Sentiment, and Recommendations\n","## ST446 Group Project\n","### Candidate Nrs: 39884, 48099, 49308, 50250"]},{"cell_type":"markdown","id":"99597c80","metadata":{},"source":["# LSA Model for Topic Modeling with LLM Representation (Groq llama3-70b-8192) Model\n","\n","This notebook applies Latent Semantic Analysis (LSA) to the Reddit comments dataset for topic modeling. LSA uncovers latent semantic patterns in a term-document matrix using Singular Value Decomposition (SVD). \n","To improve topic interpretability, topic vectors are further labeled using an LLM-based representation model (Groq's llama3-70b-8192). "]},{"cell_type":"markdown","id":"8e358cab","metadata":{},"source":["### Dataproc Cluster Setup and Initialization Actions\n","\n","Before running PySpark notebooks on Google Cloud Platform (GCP), we perform a series of steps to initialize the environment. This includes creating a GCS bucket to store resources, uploading setup scripts, and launching a Dataproc cluster configured for scalable distributed processing.\n","\n","#### Create a GCS Bucket\n","\n","We first create a Google Cloud Storage (GCS) bucket where initialization scripts are stored:\n","\n","```bash\n","gsutil mb gs://st446-gp-sm\n","```\n","#### Upload Initialization Script\n","\n","We upload a script (`my_actions.sh`) to the bucket. This script runs during cluster startup to install additional Python packages or configure the environment as needed.\n","\n","```bash\n","gsutil cp my_actions.sh gs://st446-gp-sm\n","```\n","#### Create Dataproc Cluster with Custom Settings\n","\n","We launch a Dataproc cluster optimized for parallel NLP workloads. Specs include:\n","\n","- 2 worker nodes (`n2-standard-4`) for higher memory tasks\n","- Increased executor and driver memory for large matrix computations (e.g., SVD)\n","- 64 shuffle partitions for better load distribution\n","- Initialization script applied automatically on startup\n","\n","```bash\n","gcloud dataproc clusters create st446-cluster-gp-sm \\\n","  --enable-component-gateway \\\n","  --public-ip-address \\\n","  --region=europe-west1 \\\n","  --master-machine-type=n2-standard-4 \\\n","  --master-boot-disk-size=100 \\\n","  --num-workers=2 \\\n","  --worker-machine-type=n2-standard-4 \\\n","  --worker-boot-disk-size=200 \\\n","  --image-version=2.2-debian12 \\\n","  --optional-components=JUPYTER \\\n","  --metadata='PIP_PACKAGES=sklearn nltk pandas numpy' \\\n","  --initialization-actions='gs://st446-gp-sm/my_actions.sh' \\\n","  --project=capstone-data-1-wto \\\n","  --properties='spark:spark.driver.memory=8g,spark:spark.executor.memory=6g,spark:spark.executor.cores=2,spark:spark.executor.instances=2,spark:spark.yarn.executor.memoryOverhead=1024,spark:spark.sql.shuffle.partitions=64'\n","```"]},{"cell_type":"code","execution_count":1,"id":"3ff4e8e1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/default/lib/python3.11/site-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/default/lib/python3.11/site-packages (from gensim) (1.11.4)\n","Collecting smart-open>=1.8.1 (from gensim)\n","  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n","Collecting wrapt (from smart-open>=1.8.1->gensim)\n","  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m168.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n","Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n","Installing collected packages: wrapt, smart-open, gensim\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3/3\u001B[0m [gensim]2m2/3\u001B[0m [gensim]\n","\u001B[1A\u001B[2KSuccessfully installed gensim-4.3.3 smart-open-7.1.0 wrapt-1.17.2\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0mCollecting groq\n","  Downloading groq-0.24.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/default/lib/python3.11/site-packages (from groq) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/default/lib/python3.11/site-packages (from groq) (1.8.0)\n","Collecting httpx<1,>=0.23.0 (from groq)\n","  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/default/lib/python3.11/site-packages (from groq) (2.11.3)\n","Requirement already satisfied: sniffio in /opt/conda/default/lib/python3.11/site-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/conda/default/lib/python3.11/site-packages (from groq) (4.13.2)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/default/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n","Requirement already satisfied: certifi in /opt/conda/default/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n","  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n","Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n","  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/default/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/default/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/default/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n","Downloading groq-0.24.0-py3-none-any.whl (127 kB)\n","Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n","Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n","Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n","Installing collected packages: h11, httpcore, httpx, groq\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4/4\u001B[0m [groq][32m3/4\u001B[0m [groq]\n","\u001B[1A\u001B[2KSuccessfully installed groq-0.24.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install gensim\n","!pip install groq"]},{"cell_type":"code","execution_count":2,"id":"b4383161","metadata":{},"outputs":[],"source":["# Import libraries used in this notebook\n","import zipfile\n","import os\n","import re\n","import hashlib\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","import string\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import string\n","import groq\n","from pyspark.sql import SparkSession\n","import pyspark.sql.functions as sql_f \n","from pyspark.ml.feature import CountVectorizer\n","from pyspark.ml.clustering import LDA\n","from time import time\n","from pyspark.sql.functions import udf, col\n","from pyspark.sql.types import IntegerType\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n","import numpy as np\n","import subprocess\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.linalg import DenseVector\n","from sklearn.decomposition import TruncatedSVD\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","from time import time\n","lemmatizer = WordNetLemmatizer()\n","from pyspark.ml.feature import StopWordsRemover, Tokenizer, CountVectorizer, IDF\n","from pyspark.sql.functions import col, lower, regexp_replace, udf, size\n","from pyspark.sql.types import ArrayType, StringType\n","from pyspark.mllib.linalg import Vectors as OldVectors\n","from pyspark.mllib.linalg.distributed import RowMatrix\n","from gensim.models.coherencemodel import CoherenceModel\n","from gensim.corpora.dictionary import Dictionary"]},{"cell_type":"markdown","id":"8261a589","metadata":{},"source":["# Cluster Specifications"]},{"cell_type":"code","execution_count":3,"id":"767b2699","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["vCPUs per executor: 2\n","RAM per executor: 6g\n"]},{"name":"stderr","output_type":"stream","text":["25/05/05 22:48:51 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n","25/05/05 22:48:51 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n"]}],"source":["spark_conf = spark.sparkContext.getConf()\n","executor_cores = spark_conf.get(\"spark.executor.cores\")\n","executor_memory = spark_conf.get(\"spark.executor.memory\")\n","print(f\"vCPUs per executor: {executor_cores}\")\n","print(f\"RAM per executor: {executor_memory}\")"]},{"cell_type":"markdown","id":"b8b0d162","metadata":{},"source":["## Data Loading\n","1. Download the csv file from https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n","2. Upload the zip file to bucket in cloud storage\n","3. SSH into the masternode and move file from cloud storage to masternode via: gsutil cp gs://<your-bucket-name>/archive.zip /home/freya_nagel\n","4. Verify that it is there: ls -lh ~/archive.zip"]},{"cell_type":"code","execution_count":4,"id":"1ff92910","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 1536M  100 1536M    0     0  43.2M      0  0:00:35  0:00:35 --:--:-- 43.9MM\n"]}],"source":["# 1. Download the Kaggle dataset zip\n","!curl -L -o climate.zip \\\n","    \"https://www.kaggle.com/api/v1/datasets/download/pavellexyr/the-reddit-climate-change-dataset\""]},{"cell_type":"code","execution_count":5,"id":"44ef721b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  climate.zip\n","  inflating: the-reddit-climate-change-dataset-comments.csv  \n","  inflating: the-reddit-climate-change-dataset-posts.csv  \n","Found 4 items\n","-rw-r--r--   2 root hadoop 4111000325 2025-05-05 22:51 /the-reddit-climate-change-dataset-comments.csv\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-05 22:42 /tmp\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-05 22:47 /user\n","drwxrwxrwt   - hdfs hadoop          0 2025-05-05 22:40 /var\n"]}],"source":["# 2. Unzip it (this will extract all files, including the comments CSV)\n","!unzip -o climate.zip\n","\n","# 3. Remove any old copy in HDFS and put the comments file there\n","!hadoop fs -rm -f /the-reddit-climate-change-dataset-comments.csv\n","!hadoop fs -put the-reddit-climate-change-dataset-comments.csv /\n","\n","# 4. Verify upload\n","!hadoop fs -ls /"]},{"cell_type":"code","execution_count":6,"id":"0eeaf502","metadata":{},"outputs":[],"source":["# point to the new HDFS path\n","comments_path = \"hdfs://st446-cluster-gp-sm-m:8020/the-reddit-climate-change-dataset-comments.csv\""]},{"cell_type":"markdown","id":"11b228dd","metadata":{},"source":["### Creating Corpus Dataframes"]},{"cell_type":"code","execution_count":7,"id":"957da1dd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- type: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- subreddit.id: string (nullable = true)\n"," |-- subreddit.name: string (nullable = true)\n"," |-- subreddit.nsfw: string (nullable = true)\n"," |-- created_utc: string (nullable = true)\n"," |-- permalink: string (nullable = true)\n"," |-- body: string (nullable = true)\n"," |-- sentiment: double (nullable = true)\n"," |-- score: integer (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+-------+------------+----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","|   type|     id|subreddit.id|  subreddit.name|subreddit.nsfw|created_utc|           permalink|                body|sentiment|score|\n","+-------+-------+------------+----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","|comment|icyjgoj|       2qt49|          berlin|         false| 1655656284|https://old.reddi...|&gt;These days yo...|  -0.1077|   11|\n","|comment|f0wkbi5|       2qh6p|    conservative|         false| 1569008948|https://old.reddi...|I don't think mos...|  -0.2109|    5|\n","|comment|hgbpjwd|       2qhw9|        collapse|         false| 1634019096|https://old.reddi...|Energy demand is ...|   0.6326|    2|\n","|comment|d9ugnl0|       2cneq|        politics|         false| 1478794354|https://old.reddi...|I'm a university ...|  -0.8412|   -1|\n","|comment|fcxm7ql|       2tk0s|unpopularopinion|         false| 1578066299|https://old.reddi...|The amount of lan...|    0.144|    1|\n","+-------+-------+------------+----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n","only showing top 5 rows\n","\n"]}],"source":["schema = StructType([\n","    StructField(\"type\",           StringType(), True),\n","    StructField(\"id\",             StringType(), True),\n","    StructField(\"subreddit.id\",   StringType(), True),\n","    StructField(\"subreddit.name\", StringType(), True),\n","    StructField(\"subreddit.nsfw\", StringType(), True),\n","    StructField(\"created_utc\",    StringType(), True),\n","    StructField(\"permalink\",      StringType(), True),\n","    StructField(\"body\",           StringType(), True),\n","    StructField(\"sentiment\",      DoubleType(), True),\n","    StructField(\"score\",          IntegerType(),True)\n","])\n","\n","df = spark.read \\\n","    .option(\"header\", \"true\") \\\n","    .option(\"multiLine\", \"true\") \\\n","    .option(\"escape\", \"\\\"\") \\\n","    .schema(schema) \\\n","    .csv(comments_path)\n","\n","df = df.repartition(64)\n","df.printSchema()\n","df.show(5)"]},{"cell_type":"markdown","id":"e20626da","metadata":{},"source":["# For 20% Data"]},{"cell_type":"markdown","id":"966bd8ba","metadata":{},"source":["## Saple Data Set Creation and Specifications"]},{"cell_type":"code","execution_count":8,"id":"a3702412","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:=======================================================> (62 + 2) / 64]\r"]},{"name":"stdout","output_type":"stream","text":["Number of partitions: 64\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 8:=======================================================> (62 + 2) / 64]\r"]},{"name":"stdout","output_type":"stream","text":["Number of comments in sample: 920360\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Sample\n","sample_df = df.sample(withReplacement=False, fraction=0.2, seed=42)\n","sample_df = sample_df.repartition(64)\n","print(f\"Number of partitions: {sample_df.rdd.getNumPartitions()}\")\n","# Print number of comments\n","num_comments = sample_df.count()\n","print(f\"Number of comments in sample: {num_comments}\")"]},{"cell_type":"code","execution_count":9,"id":"6669ed4d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["479.6 M  959.2 M  /tmp/sampled_data_size_check\r\n"]}],"source":["temp_path = \"hdfs:///tmp/sampled_data_size_check\"\n","sample_df.write.mode(\"overwrite\").parquet(temp_path)\n","!hadoop fs -du -s -h /tmp/sampled_data_size_check"]},{"cell_type":"markdown","id":"bc0e1844","metadata":{},"source":["## Data Pre-Processing"]},{"cell_type":"code","execution_count":10,"id":"25f76994","metadata":{},"outputs":[],"source":["# Start of runtime\n","start_time = time()"]},{"cell_type":"code","execution_count":11,"id":"1e5a95c5","metadata":{},"outputs":[],"source":["custom_stopwords = set([\n","    \"lt\", \"gt\", \"ref\", \"quot\", \"cite\", \"br\", \"amp\", \"https\", \"http\", \"urlhttps\", \"urlhttp\", \n","    \"file\", \"image\", \"jpg\", \"png\", \"gif\", \"svg\", \"thumb\", \"px\", \"category\", \"url\", \"external\", \n","    \"link\", \"source\", \"web\", \"cite\", \"reference\", \"reflist\", \"main\", \"article\", \"seealso\", \n","    \"further\", \"infobox\", \"template\", \"navbox\", \"redirect\", \"harvnb\", \"isbn\", \"doi\", \"pmid\", \n","    \"ssrn\", \"jstor\", \"bibcode\", \"arxiv\", \"ol\", \"hdl\", \"wikidata\", \"wiki\", \"math\", \"sup\", \"sub\", \n","    \"nbsp\", \"equation\", \"displaystyle\", \"begin\", \"end\", \"left\", \"right\", \"sqrt\", \"frac\", \"sum\", \n","    \"prod\", \"int\", \"lim\", \"rightarrow\", \"infty\", \"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \n","    \"zeta\", \"eta\", \"theta\", \"iota\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"xi\", \"omicron\", \"pi\", \"rho\", \n","    \"sigma\", \"tau\", \"upsilon\", \"phi\", \"chi\", \"psi\", \"omega\", \"mathrm\", \"mathbb\", \"mathcal\", \n","    \"mathbf\", \"cdots\", \"ldots\", \"vdots\", \"ddots\", \"forall\", \"exists\", \"in\", \"ni\", \"subset\", \n","    \"subseteq\", \"supset\", \"supseteq\", \"emptyset\", \"cap\", \"cup\", \"setminus\", \"not\", \"times\", \n","    \"div\", \"cdot\", \"pm\", \"mp\", \"oplus\", \"otimes\", \"odot\", \"leq\", \"geq\", \"neq\", \"approx\", \n","    \"aligncenter\", \"fontsize\", \"alignright\", \"alignleft\", \"textalign\", \"bold\", \"italic\", \n","    \"underline\", \"strikethrough\", \"lineheight\", \"padding\", \"margin\", \"width\", \"height\", \"float\", \n","    \"clear\", \"border\", \"background\", \"color\", \"font\", \"family\", \"size\", \"weight\", \"style\", \n","    \"decoration\", \"verticalalign\", \"textindent\", \"pre-wrap\", \"nowrap\", \"valign\", \"bgcolor\", \n","    \"style\", \"class\", \"id\", \"width\", \"height\", \"align\", \"border\", \"cellpadding\", \"cellspacing\", \n","    \"colspan\", \"rowspan\", \"nowrap\", \"target\", \"rel\", \"hreflang\", \"title\", \"alt\", \"src\", \"dir\", \n","    \"lang\", \"type\", \"name\", \"value\", \"readonly\", \"multiple\", \"onclick\", \"onmousedown\", \n","    \"onmouseup\", \"onmouseover\", \"onmouseout\", \"onload\", \"onunload\", \"onsubmit\", \"onreset\", \n","    \"onfocus\", \"onblur\", \"onkeydown\", \"onkeyup\", \"onkeypress\", \"onerror\", \"infobox\", \"caption\", \n","    \"cite\", \"dmy\", \"mdy\", \"date\", \"archive\", \"www\", \"com\", \"org\", \"access\", \"ndash\", \"sfn\", \"dts\", \"vauthors\", \"mvar\", \n","    \"ipaslink\", \"ipa\", \"iii\", \"ibn\", \"first\", \"last\", \"also\", \"html\", \"use\", \"publisher\", \"year\", \"one\", \n","    \"page\", \"new\", \"trek\", \"ipablink\", \"similar\", \"usual\", \"two\", \"abbr\", \"used\", \"est\", \"ibm\", \"first1\",\n","    \"first2\", \"last1\", \"last2\", \"free\", \"pdf\"\n","])"]},{"cell_type":"code","execution_count":12,"id":"ef1c1cc4","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 22:58:59 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n","[Stage 27:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|          body_clean|        final_tokens|\n","+--------------------+--------------------+\n","|sacrificing the f...|[sacrificing, fut...|\n","|lol you actually ...|[lol, actually, g...|\n","|global warming er...|[global, warming,...|\n","| gt action on cli...|[action, climate,...|\n","|meterologists hav...|[meterologists, s...|\n","+--------------------+--------------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# 1. Clean text\n","df_clean = (\n","    sample_df\n","      .withColumn(\"body_clean\", lower(col(\"body\")))\n","      .withColumn(\"body_clean\", regexp_replace(\"body_clean\", r\"http\\S+\", \"\"))  \n","      .withColumn(\"body_clean\", regexp_replace(\"body_clean\", r\"[^a-z\\s]\", \" \"))  \n","      .withColumn(\"body_clean\", regexp_replace(\"body_clean\", r\"\\s+\", \" \"))     \n",")\n","\n","# 2. Tokenize\n","tokenizer = Tokenizer(inputCol=\"body_clean\", outputCol=\"tokens\")\n","df_tokens = tokenizer.transform(df_clean)\n","\n","# 3. Remove stopwords\n","default_stops = StopWordsRemover.loadDefaultStopWords(\"english\")\n","combined_stops = list(set(default_stops) | custom_stopwords)\n","\n","remover = StopWordsRemover(\n","    inputCol=\"tokens\",\n","    outputCol=\"filtered\",\n","    stopWords=combined_stops\n",")\n","df_no_stop = remover.transform(df_tokens)\n","\n","# 4. Lemmatize\n","def lemmatize_tokens(tokens):\n","    return [lemmatizer.lemmatize(token) for token in tokens if len(token) > 2]\n","\n","lemmatize_udf = udf(lemmatize_tokens, ArrayType(StringType()))\n","\n","df_lemmatized = df_no_stop.withColumn(\"lemmatized_tokens\", lemmatize_udf(col(\"filtered\")))\n","\n","# 5. Filter out very short tokens\n","df_final = df_lemmatized.withColumn(\"final_tokens\", lemmatize_udf(col(\"lemmatized_tokens\")))\n","\n","df_final.select(\"body_clean\", \"final_tokens\").show(5)"]},{"cell_type":"code","execution_count":13,"id":"37efb251","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 44:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|        final_tokens|            Features|\n","+--------------------+--------------------+\n","|[sacrificing, fut...|(10000,[0,1,3,21,...|\n","|[lol, actually, g...|(10000,[0,1,2,3,7...|\n","|[global, warming,...|(10000,[0,1,24,50...|\n","|[action, climate,...|(10000,[0,1,5,6,7...|\n","|[meterologists, s...|(10000,[0,1,29,39...|\n","+--------------------+--------------------+\n","only showing top 5 rows\n","\n","Sample vocab size: 10000\n","First 20 sample-vocab entries: ['climate', 'change', 'people', 'like', 'think', 'thing', 'even', 'get', 'make', 'world', 'need', 'much', 'way', 'year', 'know', 'going', 'want', 'time', 'say', 'issue']\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Vectorize on the sample\n","cv = CountVectorizer(\n","    inputCol=\"final_tokens\",\n","    outputCol=\"Features\",\n","    minDF=100,\n","    vocabSize=10000\n",")\n","cv_model = cv.fit(df_final)\n","df_tf = cv_model.transform(df_final)\n","df_tf.select(\"final_tokens\", \"Features\").show(5)\n","df_tf.cache()\n","\n","vocab_sample = cv_model.vocabulary\n","print(f\"Sample vocab size: {len(vocab_sample)}\")\n","print(\"First 20 sample-vocab entries:\", vocab_sample[:20])"]},{"cell_type":"markdown","id":"42619ba9","metadata":{},"source":["# Topic Modeling"]},{"cell_type":"markdown","id":"c540a7a6","metadata":{},"source":["## LSA"]},{"cell_type":"code","execution_count":14,"id":"d6beaf1b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Convert DataFrame column to RDD of dense vectors and repartition\n","dense_rdd = df_tf.select(\"Features\") \\\n","    .rdd \\\n","    .map(lambda row: OldVectors.dense(row[\"Features\"].toArray())) \\\n","    .repartition(64)\n","\n","# Cache\n","dense_rdd = dense_rdd.cache()\n","dense_rdd.count()\n","\n","# Create RowMatrix for SVD\n","mat = RowMatrix(dense_rdd)"]},{"cell_type":"code","execution_count":15,"id":"a1b927b9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 23:11:07 WARN RowMatrix: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n","25/05/05 23:11:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.arpack.JNIARPACK\n","25/05/05 23:44:27 WARN RowMatrix: The input data was not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"]}],"source":["# Number of topics/components\n","num_topics = 7\n","\n","# Compute SVD\n","svd = mat.computeSVD(num_topics, computeU=True)\n","\n","U = svd.U  # Document-topic matrix\n","s = svd.s  # Singular values (importance of topics)\n","V = svd.V  # Word-topic matrix\n"]},{"cell_type":"code","execution_count":16,"id":"b0fb63c1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Runtime: 45.64 minutes\n"]}],"source":["# End of Runtime\n","end_time = time()\n","runtime_minutes = (end_time - start_time) / 60\n","print(f\"Runtime: {runtime_minutes:.2f} minutes\")"]},{"cell_type":"code","execution_count":17,"id":"0ab40d04","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Topic 1: ['climate', 'change', 'people', 'like', 'think', 'thing', 'even', 'get', 'make', 'world']\n","\n","Topic 2: ['climate', 'change', 'warming', 'global', 'temperature', 'scientist', 'science', 'earth', 'carbon', 'emission']\n","\n","Topic 3: ['people', 'change', 'climate', 'think', 'thing', 'like', 'believe', 'want', 'someone', 'know']\n","\n","Topic 4: ['human', 'warming', 'like', 'earth', 'temperature', 'thing', 'global', 'science', 'world', 'year']\n","\n","Topic 5: ['like', 'think', 'trump', 'thing', 'know', 'science', 'biden', 'say', 'really', 'even']\n","\n","Topic 6: ['biden', 'science', 'people', 'warming', 'global', 'temperature', 'human', 'year', 'scientist', 'earth']\n","\n","Topic 7: ['science', 'trump', 'government', 'tax', 'state', 'party', 'country', 'policy', 'republican', 'issue']\n"]}],"source":["V_np = np.array(V.toArray())\n","vocab = cv_model.vocabulary\n","\n","for topic_idx in range(V_np.shape[1]):  \n","    topic_weights = V_np[:, topic_idx]  \n","    top_indices = topic_weights.argsort()[-10:][::-1]\n","    top_words = [vocab[i] for i in top_indices]\n","    print(f\"\\nTopic {topic_idx + 1}: {top_words}\")"]},{"cell_type":"markdown","id":"0b5eba27","metadata":{},"source":["### LSA Discussion\n","\n","Topic modeling using LSA had several challenges. Although it effectively reduces dimensionality using Singular Value Decomposition (SVD), the resulting topics showed significant word overlap—terms like *climate*, *change*, *people*, and *like* appeared in multiple topics. This indicates that LSA produced less distinct and more blended themes.\n","\n","From a computational perspective, LSA was not efficient. It required dense matrix representations and lacked native sparse matrix support in PySpark, which led to high memory usage. Even after limiting vocabulary size and sampling 20% of the dataset, we encountered memory errors and executor crashes during SVD. 64 partitions improved stability, but runtime remained high. "]},{"cell_type":"markdown","id":"9107a51e","metadata":{},"source":["## Evaluation of Model using different Metrics"]},{"cell_type":"code","execution_count":18,"id":"d39df1fd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_52 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_58 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_9 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_29 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_5 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_15 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_44 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_44 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_33 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_8 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_60 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_20 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_32 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_48 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_56 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_25 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_37 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_22 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_17 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_49 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_36 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_7 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_21 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_45 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_63 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_45 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_62 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_37 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_33 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_41 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_14 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_53 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_35 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_40 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_2 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_6 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_11 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_23 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_13 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_28 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_18 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_31 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_0 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_12 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_29 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_61 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_26 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_57 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_41 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_40 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_0 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_49 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_57 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_53 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_56 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_19 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_59 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_24 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_10 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_16 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_1 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_4 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_48 !\n","25/05/05 23:44:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_52 !\n","25/05/05 23:44:47 WARN YarnAllocator: Container from a bad node: container_1746484789411_0001_01_000012 on host: st446-cluster-gp-sm-w-1.europe-west1-b.c.capstone-data-1-wto.internal. Exit status: 143. Diagnostics: [2025-05-05 23:44:46.962]Container killed on request. Exit code is 143\n","[2025-05-05 23:44:46.962]Container exited with a non-zero exit code 143. \n","[2025-05-05 23:44:46.962]Killed by external signal\n",".\n","25/05/05 23:44:47 ERROR YarnScheduler: Lost executor 6 on st446-cluster-gp-sm-w-1.europe-west1-b.c.capstone-data-1-wto.internal: Container from a bad node: container_1746484789411_0001_01_000012 on host: st446-cluster-gp-sm-w-1.europe-west1-b.c.capstone-data-1-wto.internal. Exit status: 143. Diagnostics: [2025-05-05 23:44:46.962]Container killed on request. Exit code is 143\n","[2025-05-05 23:44:46.962]Container exited with a non-zero exit code 143. \n","[2025-05-05 23:44:46.962]Killed by external signal\n",".\n","25/05/05 23:44:47 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1746484789411_0001_01_000012 on host: st446-cluster-gp-sm-w-1.europe-west1-b.c.capstone-data-1-wto.internal. Exit status: 143. Diagnostics: [2025-05-05 23:44:46.962]Container killed on request. Exit code is 143\n","[2025-05-05 23:44:46.962]Container exited with a non-zero exit code 143. \n","[2025-05-05 23:44:46.962]Killed by external signal\n",".\n","25/05/05 23:44:47 WARN TaskSetManager: Lost task 0.0 in stage 264.0 (TID 3925) (st446-cluster-gp-sm-w-1.europe-west1-b.c.capstone-data-1-wto.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746484789411_0001_01_000012 on host: st446-cluster-gp-sm-w-1.europe-west1-b.c.capstone-data-1-wto.internal. Exit status: 143. Diagnostics: [2025-05-05 23:44:46.962]Container killed on request. Exit code is 143\n","[2025-05-05 23:44:46.962]Container exited with a non-zero exit code 143. \n","[2025-05-05 23:44:46.962]Killed by external signal\n",".\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_6 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_43 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_36 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_13 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_12 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_38 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_10 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_23 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_31 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_27 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_61 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_47 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_16 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_39 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_54 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_19 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_55 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_14 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_30 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_35 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_8 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_25 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_50 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_26 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_51 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_11 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_47 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_30 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_5 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_54 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_46 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_62 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_27 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_4 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_21 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_63 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_55 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_60 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_15 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_3 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_2 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_34 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_20 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_32 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_22 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_42 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_24 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_59 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_34 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_3 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_42 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_39 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_46 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_9 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_43 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_7 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_51 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_50 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_106_18 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_58 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_1 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_17 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_28 !\n","25/05/05 23:44:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_117_38 !\n"]},{"name":"stderr","output_type":"stream","text":["25/05/05 23:44:49 WARN YarnAllocator: Container from a bad node: container_1746484789411_0001_01_000018 on host: st446-cluster-gp-sm-w-0.europe-west1-b.c.capstone-data-1-wto.internal. Exit status: 143. Diagnostics: [2025-05-05 23:44:49.355]Container killed on request. Exit code is 143\n","[2025-05-05 23:44:49.355]Container exited with a non-zero exit code 143. \n","[2025-05-05 23:44:49.356]Killed by external signal\n",".\n","25/05/05 23:44:49 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 7 for reason Container from a bad node: container_1746484789411_0001_01_000018 on host: st446-cluster-gp-sm-w-0.europe-west1-b.c.capstone-data-1-wto.internal. Exit status: 143. Diagnostics: [2025-05-05 23:44:49.355]Container killed on request. Exit code is 143\n","[2025-05-05 23:44:49.355]Container exited with a non-zero exit code 143. \n","[2025-05-05 23:44:49.356]Killed by external signal\n",".\n","25/05/05 23:44:49 ERROR YarnScheduler: Lost executor 7 on st446-cluster-gp-sm-w-0.europe-west1-b.c.capstone-data-1-wto.internal: Container from a bad node: container_1746484789411_0001_01_000018 on host: st446-cluster-gp-sm-w-0.europe-west1-b.c.capstone-data-1-wto.internal. Exit status: 143. Diagnostics: [2025-05-05 23:44:49.355]Container killed on request. Exit code is 143\n","[2025-05-05 23:44:49.355]Container exited with a non-zero exit code 143. \n","[2025-05-05 23:44:49.356]Killed by external signal\n",".\n","25/05/05 23:44:49 WARN TaskSetManager: Lost task 0.1 in stage 264.0 (TID 3926) (st446-cluster-gp-sm-w-0.europe-west1-b.c.capstone-data-1-wto.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746484789411_0001_01_000018 on host: st446-cluster-gp-sm-w-0.europe-west1-b.c.capstone-data-1-wto.internal. Exit status: 143. Diagnostics: [2025-05-05 23:44:49.355]Container killed on request. Exit code is 143\n","[2025-05-05 23:44:49.355]Container exited with a non-zero exit code 143. \n","[2025-05-05 23:44:49.356]Killed by external signal\n",".\n","                                                                                \r"]}],"source":["sampled_docs = df_final.select(\"final_tokens\").sample(withReplacement=False, fraction=0.01, seed=42)\n","docs_tokens = sampled_docs.rdd.map(lambda row: row[0]).collect()\n","\n","# top words per topic from V\n","top_words = [[vocab[i] for i in topic.argsort()[-10:][::-1]] for topic in V_np.T]"]},{"cell_type":"code","execution_count":19,"id":"8bb5ee29","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CV Coherence Score: 0.5012\n"]}],"source":["# Gensim dictionary and corpus from tokenized docs\n","dictionary = Dictionary(docs_tokens)\n","corpus = [dictionary.doc2bow(doc) for doc in docs_tokens]\n","\n","cm = CoherenceModel(topics=top_words, texts=docs_tokens, dictionary=dictionary, coherence='c_v')\n","cv_score = cm.get_coherence()\n","\n","print(f\"CV Coherence Score: {cv_score:.4f}\")"]},{"cell_type":"code","execution_count":20,"id":"15d8167a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["UMass Coherence Score: -1.8095\n"]}],"source":["cm_umass = CoherenceModel(\n","    topics=top_words,\n","    texts=docs_tokens,\n","    dictionary=dictionary,\n","    coherence='u_mass'\n",")\n","umass_score = cm_umass.get_coherence()\n","print(f\"UMass Coherence Score: {umass_score:.4f}\")"]},{"cell_type":"code","execution_count":21,"id":"3bda16e4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["C_NPMI Coherence Score: 0.0333\n"]}],"source":["cm_cnpmi = CoherenceModel(\n","    topics=top_words,\n","    texts=docs_tokens,\n","    dictionary=dictionary,\n","    coherence='c_npmi'\n",")\n","cnpmi_score = cm_cnpmi.get_coherence()\n","print(f\"C_NPMI Coherence Score: {cnpmi_score:.4f}\")"]},{"cell_type":"code","execution_count":22,"id":"9a957dcf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Topic Diversity Score: 0.5143\n"]}],"source":["def topic_diversity(top_words):\n","    unique_words = set()\n","    for topic in top_words:\n","        unique_words.update(topic)\n","    return len(unique_words) / (len(top_words) * len(top_words[0]))\n","\n","diversity_score = topic_diversity(top_words)\n","print(f\"Topic Diversity Score: {diversity_score:.4f}\")"]},{"cell_type":"code","execution_count":26,"id":"e70c81a6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 277:======================================================>(63 + 1) / 64]\r"]},{"name":"stdout","output_type":"stream","text":["Topic Size Imbalance (Max/Min, filtered): 27.47\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Coded this with the help of ChatGPT\n","# document-topic matrix U\n","u_rows = svd.U.rows  \n","\n","# dominant topic index per document\n","dominant_rdd = u_rows.map(lambda vec: int(vec.toArray().argmax()))\n","\n","# Count documents per topic\n","counts = dominant_rdd.countByValue()\n","\n","# Compute imbalance ratio\n","vals = list(counts.values())\n","imbalance = max(vals) / min(vals)\n","print(f\"Topic Size Imbalance (Max/Min, filtered): {imbalance:.2f}\")"]},{"cell_type":"markdown","id":"d0ed6ca4","metadata":{},"source":["## LLM (Groq llama3-70b-8192) representation model\n","To improve human interpretability and create more sound and clean topic labels, we prompt Groq llama3-70b-8192 via an API to provide us with topic labels."]},{"cell_type":"code","execution_count":24,"id":"27988d0a","metadata":{},"outputs":[],"source":["import groq\n","\n","# Set Groq API Key\n","GROQ_API_KEY = \"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\"\n","client = groq.Groq(api_key=GROQ_API_KEY)\n","\n","# Define prompt template\n","prompt_template = \"\"\"\n","I have a topic described by the following keywords:\n","{keywords}\n","\n","Based on these keywords, generate a short and descriptive topic label of at most 5 words.\n","Make sure the output follows this format:\n","topic: <topic label>\n","\"\"\""]},{"cell_type":"code","execution_count":25,"id":"b4a70aef","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== GROQ Labeled LSA Topics ===\n","Topic 0: People's Thoughts on Climate\n","Keywords: climate, change, people, like, think, thing, even, get, make, world\n","\n","Topic 1: Global Climate Change Science\n","Keywords: climate, change, warming, global, temperature, scientist, science, earth, carbon, emission\n","\n","Topic 2: People's Beliefs on Climate\n","Keywords: people, change, climate, think, thing, like, believe, want, someone, know\n","\n","Topic 3: Global Warming Science\n","Keywords: human, warming, like, earth, temperature, thing, global, science, world, year\n","\n","Topic 4: Politicians' Views on Science\n","Keywords: like, think, trump, thing, know, science, biden, say, really, even\n","\n","Topic 5: Biden on Global Warming\n","Keywords: biden, science, people, warming, global, temperature, human, year, scientist, earth\n","\n","Topic 6: Trump's GOP Tax Policy\n","Keywords: science, trump, government, tax, state, party, country, policy, republican, issue\n","\n"]}],"source":["# Labeling function using Groq API\n","def get_groq_label(keywords):\n","    prompt = prompt_template.format(keywords=\", \".join(keywords))\n","    response = client.chat.completions.create(\n","        model=\"llama3-70b-8192\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    )\n","    raw_text = response.choices[0].message.content.strip()\n","    match = re.search(r\"topic:\\s*(.+)\", raw_text, re.IGNORECASE)\n","    return match.group(1).strip() if match else raw_text\n","\n","# Apply labeling to LSA topics\n","lsa_topic_labels = {}\n","\n","print(\"=== GROQ Labeled LSA Topics ===\")\n","for topic_id, words in enumerate(top_words):\n","    label = get_groq_label(words[:10])\n","    lsa_topic_labels[topic_id] = label\n","    print(f\"Topic {topic_id}: {label}\")\n","    print(\"Keywords:\", \", \".join(words[:10]))\n","    print()"]},{"cell_type":"markdown","id":"7ea1251b","metadata":{},"source":["### LSA + LLM Topic Interpretation\n","\n","The topics generated through Latent Semantic Analysis (LSA), when enriched with GROQ's LLM-based labeling, demonstrate strong thematic coherence. Topics range from public opinion on climate change (Topics 0 and 2) to scientific discourse on global warming (Topics 1 and 3), and extend to political framing—highlighting figures like Biden and Trump (Topics 4 to 6). The LLM-generated labels effectively capture the semantic intent of each topic, improving interpretability and reducing ambiguity in downstream analysis.\n"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}